<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="http://www.micahlerner.com/feed.xml" rel="self" type="application/atom+xml" /><link href="http://www.micahlerner.com/" rel="alternate" type="text/html" /><updated>2021-11-05T17:46:58-07:00</updated><id>http://www.micahlerner.com/feed.xml</id><title type="html">www.micahlerner.com</title><author><name>Micah</name></author><entry><title type="html">Rudra: Finding Memory Safety Bugs in Rust at the Ecosystem Scale</title><link href="http://www.micahlerner.com/2021/10/31/rudra-finding-memory-safety-bugs-in-rust-at-the-ecosystem-scale.html" rel="alternate" type="text/html" title="Rudra: Finding Memory Safety Bugs in Rust at the Ecosystem Scale" /><published>2021-10-31T00:00:00-07:00</published><updated>2021-10-31T00:00:00-07:00</updated><id>http://www.micahlerner.com/2021/10/31/rudra-finding-memory-safety-bugs-in-rust-at-the-ecosystem-scale</id><content type="html" xml:base="http://www.micahlerner.com/2021/10/31/rudra-finding-memory-safety-bugs-in-rust-at-the-ecosystem-scale.html">&lt;p&gt;&lt;em&gt;The papers over the next few weeks will be from &lt;a href=&quot;https://sosp2021.mpi-sws.org/&quot;&gt;SOSP&lt;/a&gt;. As always, feel free to reach out on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt; with feedback or suggestions about papers to read! These paper reviews can &lt;a href=&quot;https://www.getrevue.co/profile/systems-weekly&quot;&gt;be delivered weekly to your inbox&lt;/a&gt;, or you can subscribe to the &lt;a href=&quot;https://www.micahlerner.com/feed.xml&quot;&gt;Atom feed&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3477132.3483570&quot;&gt;Rudra: Finding Memory Safety Bugs in Rust at the Ecosystem Scale&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This week’s paper is about &lt;em&gt;Rudra&lt;/em&gt;, a system&lt;label for=&quot;os&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;os&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The Rudra code itself is &lt;a href=&quot;https://github.com/sslab-gatech/Rudra&quot;&gt;open source on Github&lt;/a&gt;. &lt;/span&gt; for finding memory safety bugs in code written with the &lt;a href=&quot;https://www.rust-lang.org/&quot;&gt;Rust programming language&lt;/a&gt;. Rust is used for many purposes, although it is particularly popular for lower level systems programming - the language’s approach to memory management allows the compiler to eliminate many common types of memory management issues, in turn improving security. As a result, Rust is used across many high-profile open source projects where security matters, including the Mozilla &lt;a href=&quot;https://github.com/servo/servo/&quot;&gt;Servo engine&lt;/a&gt;, the open-source &lt;a href=&quot;https://firecracker-microvm.github.io/&quot;&gt;Firecracker MicroVM technology&lt;/a&gt; used in AWS Lambda/Fargate&lt;label for=&quot;firecracker&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;firecracker&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Firecracker is also used in many other open source projects - see my &lt;a href=&quot;/2021/06/17/firecracker-lightweight-virtualization-for-serverless-applications.html&quot;&gt;previous paper review&lt;/a&gt; on Firecracker for more details. &lt;/span&gt;, and the &lt;a href=&quot;https://fuchsia.dev/fuchsia-src/get-started/learn&quot;&gt;Fuschia operating system&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Unfortunately, it is not possible to implement every functionality with code that obeys the language’s rules around memory management. To address this gap, Rust includes an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsafe&lt;/code&gt; tag that allows code to suspend some of the rules, albeit within well defined blocks of code. While &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsafe&lt;/code&gt; sections of Rust code are generally reviewed closely, the language construct can lead to subtle bugs that compromise the security Rust code.&lt;/p&gt;

&lt;p&gt;The goal of Rudra is automatically evaluating these &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsafe&lt;/code&gt; sections of code to find security issues. Rudra has achieved remarkable success - at the time of the paper’s publication, the system had identified 76 CVEs and ~52% of the memory safety bugs in the official Rust security advisory database, &lt;a href=&quot;https://rustsec.org/&quot;&gt;RustSec&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;what-are-the-papers-contributions&quot;&gt;What are the paper’s contributions?&lt;/h2&gt;

&lt;p&gt;The Rudra paper makes three primary contributions: it describes scalable algorithms for finding memory safety bugs in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsafe&lt;/code&gt; Rust code, implements the algorithms in the open source &lt;a href=&quot;https://github.com/sslab-gatech/Rudra&quot;&gt;Rudra project&lt;/a&gt;, and demonstrates using the project to find bugs in existing open source code.&lt;/p&gt;

&lt;h2 id=&quot;safe-rust&quot;&gt;Safe Rust&lt;/h2&gt;

&lt;p&gt;In order to understand the memory safety issues that Rudra detects, it is important to understand how Rust provides memory safety guarantees at compile time and the idea of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsafe&lt;/code&gt; Rust. For those familar with these topics, skipping to “Pitfalls of Safe Rust” might make sense.&lt;/p&gt;

&lt;h3 id=&quot;language-features&quot;&gt;Language features&lt;/h3&gt;

&lt;p&gt;To provide memory safety guarantees at compile time, Rust uses: &lt;em&gt;ownership&lt;/em&gt;, &lt;em&gt;borrowing&lt;/em&gt;, and &lt;em&gt;aliasising xor mutability&lt;/em&gt;&lt;label for=&quot;community&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;community&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;There are many great posts from the Rust community that explore these topics - feel free to DM me on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt; with more as I am far from a Rust expert! While writing this paper review, I read the amazing &lt;a href=&quot;https://doc.rust-lang.org/book/ch04-00-understanding-ownership.html&quot;&gt;Rust docs&lt;/a&gt;, &lt;a href=&quot;https://limpet.net/mbrubeck/2019/02/07/rust-a-unique-perspective.html&quot;&gt;Rust: A unique perspective&lt;/a&gt; and &lt;a href=&quot;https://onesignal.com/blog/thread-safety-rust/&quot;&gt;Thread safety and Learning in Rust&lt;/a&gt;. &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Ownership&lt;/em&gt;, according to the &lt;a href=&quot;https://doc.rust-lang.org/book/ch04-01-what-is-ownership.html#ownership-rules&quot;&gt;Rust documentation&lt;/a&gt; means that:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Each value in Rust has a variable that’s called its owner.&lt;/li&gt;
    &lt;li&gt;There can only be one owner at a time.&lt;/li&gt;
    &lt;li&gt;When the owner goes out of scope, the value will be dropped.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Borrowing&lt;/em&gt; allows one “to access data without taking ownership over it” - &lt;a href=&quot;https://doc.rust-lang.org/beta/rust-by-example/scope/borrow.html#borrowing&quot;&gt;Rust By Example&lt;/a&gt; has a helpful section on the topic. Integration of borrowing semantics into the language help to address problems related to accessing variables after the reference is no longer valid&lt;label for=&quot;uaf&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;uaf&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Which are exploited by &lt;a href=&quot;https://cwe.mitre.org/data/definitions/416.html&quot;&gt;Use After Free&lt;/a&gt;. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Aliasising xor mutability&lt;/em&gt; means that the language prevents, “both shared and mutable references … at the same time. This means that concurrent reads and writes are fundamentally impossible in Rust, eliminating the possibility of conventional race conditions and memory safety bugs like accessing invalid references”&lt;label for=&quot;iteratorinvalidation&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;iteratorinvalidation&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The paper cites &lt;a href=&quot;https://stackoverflow.com/questions/16904454/what-is-iterator-invalidation&quot;&gt;iterator invalidation&lt;/a&gt; as one example of accessing invalid references prevented by this approach. &lt;/span&gt;.&lt;/p&gt;

&lt;h3 id=&quot;unsafe-rust&quot;&gt;Unsafe Rust&lt;/h3&gt;

&lt;p&gt;To implement certain features (and bound undefined behavior), writers of Rust code can mark blocks with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsafe&lt;/code&gt;. From the &lt;a href=&quot;https://doc.rust-lang.org/book/ch19-01-unsafe-rust.html&quot;&gt;Rust docs&lt;/a&gt;, this allows that code block to perform a number of actions that wouldn’t be permitted otherwise, like “call unsafe functions (including C functions, compiler intrinsics, and the raw allocator)”. C doesn’t operate according to Rust’s constraints (specifically around undefined behavior), so using it inside of Rust code needs to be marked &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsafe&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;An example use case of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsafe&lt;/code&gt; is performing memory-mapped IO. Memory-mapped IO relies on mapping a file to a region of memory using &lt;a href=&quot;https://www.gnu.org/software/libc/manual/html_node/Memory_002dmapped-I_002fO.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mmap&lt;/code&gt;&lt;/a&gt;. The &lt;a href=&quot;https://github.com/danburkert/memmap-rs/blob/master/src/unix.rs#L48&quot;&gt;implementation of memory-mapping IO&lt;/a&gt; from one of the most popular Rust memory mapping libraries &lt;a href=&quot;https://docs.rs/libc/0.2.1/src/libc/.cargo/registry/src/github.com-1ecc6299db9ec823/libc-0.2.1/src/unix/mod.rs.html#291-297&quot;&gt;calls the C function mmap&lt;/a&gt;, meaning that the function must be inside of an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsafe&lt;/code&gt; block.&lt;/p&gt;

&lt;h2 id=&quot;pitfalls-of-unsafe-rust&quot;&gt;Pitfalls of Unsafe Rust&lt;/h2&gt;

&lt;p&gt;Now that we roughly know why code is marked &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsafe&lt;/code&gt;, this section moves on to the three main types of issues detected by Rudra in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsafe&lt;/code&gt; Rust code: &lt;em&gt;panic safety&lt;/em&gt;, &lt;em&gt;higher order invariant safety&lt;/em&gt;, and &lt;em&gt;propagating send/sync in generic types&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Panic safety&lt;/em&gt; is a problem that crops up in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsafe&lt;/code&gt; blocks that initialize some state with the intention of further action. If these code blocks hit a panic (which “unwinds” the current call, destroying objects along the way), the further action isn’t taken and “the destructors of the variable will run without realizing that the variable is in an inconsistent state, resulting in memory safety issues similar to uninitialized uses or double frees in C/C++.”&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Higher order invariant safety&lt;/em&gt; means that a “Rust function should execute safely for all safe inputs.” To ensure that a function operates only on arguments it can use safely (failing otherwise), Rust code can check the properties of the provided arguments. Checking arguments is made more difficult in some cases because a provided argument may be &lt;a href=&quot;https://doc.rust-lang.org/book/ch10-01-syntax.html&quot;&gt;generic&lt;/a&gt;, and the specifics about the implementation of the argument may not be available. One example of a higher order invariant is:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“passing an uninitialized buffer to a caller-provided Read implementation. Read is commonly expected to read data from one source (e.g., a file) and write into the provided buffer. However, it is perfectly valid to read the buffer under Rust’s type system. This leads to undefined behavior if the buffer contains uninitialized memory.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Propagating send/sync in generic types&lt;/em&gt; is related to two traits&lt;label for=&quot;traits&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;traits&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;&lt;a href=&quot;https://doc.rust-lang.org/book/ch10-02-traits.html&quot;&gt;Traits&lt;/a&gt; in Rust often contain shared functionality that can be mixed into code. &lt;/span&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Send&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Sync&lt;/code&gt;) used for thread safety&lt;label for=&quot;sendsync&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;sendsync&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;More information and examples of how Send and Sync are used &lt;a href=&quot;https://doc.rust-lang.org/book/ch16-04-extensible-concurrency-sync-and-send.html&quot;&gt;from the Rust docs&lt;/a&gt; and &lt;a href=&quot;https://stackoverflow.com/questions/59428096/understanding-the-send-trait&quot;&gt;StackOverflow&lt;/a&gt;. &lt;/span&gt;. The compiler can automatically determine how a Trait gets assigned Send/Sync - if all of a Trait’s properties are Send/Sync, it is safe to conclude that the Trait containing those properties implements Send/Sync itself. For other Traits (like locks), Send/Sync behavior can not be automatically passed on - one example is for a container class (like a list) that contains types that are not Send/Sync themselves. In these situations, the code must implement Send/Sync manually, leading to potential memory safety issues if the implementation uses unsafe code and is incorrect&lt;label for=&quot;incorrect&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;incorrect&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The paper notes it is possible for an implementation to be incorrect initially, &lt;em&gt;or&lt;/em&gt; for the implementation to become incorrect over time due to ongoing maintenance (made more likely by having the implementation spread out over a codebase). &lt;/span&gt; in some way.&lt;/p&gt;

&lt;h2 id=&quot;design-of-rudra&quot;&gt;Design of Rudra&lt;/h2&gt;

&lt;p&gt;This section describes the system’s &lt;em&gt;design goals&lt;/em&gt; (what the system needs to do in order to find memory safety issues in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsafe&lt;/code&gt; Rust code), as well as how Rudra is designed to achieve those goals.&lt;/p&gt;

&lt;h3 id=&quot;design-goals&quot;&gt;Design Goals&lt;/h3&gt;

&lt;p&gt;To achieve the goal of finding memory safety issues in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsafe&lt;/code&gt; Rust code, Rudra needs to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Consume metadata about Rust typing, not available at lower levels of the compiler (more on what this means later).&lt;/li&gt;
  &lt;li&gt;Analyze the entirety of the Rust ecosystem, using limited resources.&lt;/li&gt;
  &lt;li&gt;Be able to make the tradeoff between using limited resources and the precision of results. More resources can be expended in order to verify results&lt;label for=&quot;resources&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;resources&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;One way to think about this tradeoff is that Rudra aims to find paths in the code that could potentially lead to memory safety issues. Using more resources allows further verification or simulation along those paths to determine whether a code path does in fact lead to a bug. &lt;/span&gt;, leading to fewer false positives. On the other hand, Rudra aims to analyze the entirety of the Rust ecosystem on an ongoing basis, so the program should also be able to expend fewer resources and run faster, with the potential for more false positives.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rudra-components&quot;&gt;Rudra components&lt;/h3&gt;

&lt;p&gt;To achieve its design goals, Rudra implements two algorithms on intermediate representations (IR)&lt;label for=&quot;ir&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;ir&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The Rust &lt;a href=&quot;https://rustc-dev-guide.rust-lang.org/part-3-intro.html&quot;&gt;docs&lt;/a&gt; on how code is represented and compiled is really great! I highly recommend it for those interested in learning more about the internals. &lt;/span&gt; produced by the Rust compiler: an &lt;em&gt;unsafe dataflow checker&lt;/em&gt; and &lt;em&gt;send/sync variance checker&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;unsafe dataflow checker&lt;/em&gt; finds &lt;em&gt;panic safety bugs&lt;/em&gt; (which can occur if a panic happens during an unsafe section and the code is in a temporarily inconsistent state) and &lt;em&gt;higher order invariant bugs&lt;/em&gt; (which can happen if a function doesn’t, or can’t, verify passed arguments to ensure it is safe to operate on them). The algorithm checks for &lt;em&gt;lifetime bypasses&lt;/em&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsafe&lt;/code&gt; Rust code that perform logic not otherwise permitted by the compiler&lt;label for=&quot;lang&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;lang&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;What is or is not allowed by the compiler is discussed in the ‘Language Features’ section above. &lt;/span&gt; - this general category of functionality can contribute to &lt;em&gt;panic safety bugs&lt;/em&gt; or &lt;em&gt;higher order invariant bugs&lt;/em&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The algorithm models six classes of lifetime bypasses:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;uninitialized: creating uninitialized values&lt;/li&gt;
    &lt;li&gt;duplicate: duplicating the lifetime of objects (e.g., with mem::read())&lt;/li&gt;
    &lt;li&gt;write: overwriting the memory of a value&lt;/li&gt;
    &lt;li&gt;copy: memcpy()-like buffer copy&lt;/li&gt;
    &lt;li&gt;transmute: reinterpreting a type and its lifetime&lt;/li&gt;
    &lt;li&gt;ptr-to-ref : converting a pointer to a reference&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;The &lt;em&gt;send/sync variance checker&lt;/em&gt; evaluates a set of rules to determine whether a data type meets Send/Sync constraints given the usage of the data type - for example, some data types might require only Send, only Sync, both, or neither. The heuristics for performing this evaluation are described in more detail in the paper (and are also implemented in the &lt;a href=&quot;https://github.com/sslab-gatech/Rudra/blob/7949384a3514fbc1f970e5f309202b6c7a16aa48/src/analysis/send_sync_variance/strict.rs&quot;&gt;open source project&lt;/a&gt;). Once the variance checker determines whether Send/Sync are needed for a data type, it compares that to the actual implementation, raising an issue if there is a mismatch.&lt;/p&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;Rudra is implemented as a custom Rust compiler driver, meaning it hooks into the Rust compilation process:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It works as an unmodified Rust compiler when compiling dependencies and injects the analysis algorithms when compiling the target package.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/rudra/arch.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The two algorithms implemented in Rudra operate on different intermediate representations&lt;label for=&quot;ir&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;ir&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The Rust &lt;a href=&quot;https://rustc-dev-guide.rust-lang.org/part-3-intro.html&quot;&gt;docs&lt;/a&gt; on how code is represented and compiled is really great! I highly recommend it for those interested in learning more about the internals. &lt;/span&gt; (IR) of Rust code. The &lt;em&gt;unsafe dataflow checker&lt;/em&gt; runs on the HIR, which has code structure, while the &lt;em&gt;send/sync variance checker&lt;/em&gt; operates on the (MIR).&lt;/p&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;

&lt;p&gt;At the time of publication, Rudra had found 264 memory safety bugs in open source Rust packages, including 76 CVEs. To make the point about how tricky (and novel) some of these problems were to detect before Rudra found them, the paper notes that several of the issues were in the Rust standard library (which is reviewed by Rust experts).&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/rudra/eval.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;While the project had significant success finding bugs, it als has false positive rate of around 50%, although the precision is adjustable). On the other hand, most of the false positives could be quickly resolved by visual inspection according to the authors.&lt;/p&gt;

&lt;p&gt;The authors compare Rudra to other tools for finding bugs in Rust code. Rudra ran faster than commonly used fuzzers, while also finding more bugs. When applied to the same codebases as another Rust-focused tool, &lt;a href=&quot;https://github.com/rust-lang/miri&quot;&gt;Miri&lt;/a&gt;, the issues found by the two tools partially overlap (although Miri found unique bugs, indicating the approach is complementary).&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Rudra focuses on finding memory management issues in Rust code (and does so quite successfully). Importantly, when Rudra &lt;em&gt;does&lt;/em&gt; find issues, the paper notes it is relatively easier to assign ownership of fixing the root cause to the package with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsafe&lt;/code&gt; block.&lt;/p&gt;

&lt;p&gt;Even though &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsafe&lt;/code&gt; is required so Rust can support certain functionality, it is an opt-in language feature, limiting the surface area of memory management issues. This reflects a significant improvement over other languages where similarly unsafe code can be anywhere in a code base. While there is still work to be done on changing how the system detects and limits false positives, I am hopeful that it continues to evolve alongside the growing Rust ecosystem.&lt;/p&gt;

&lt;p&gt;As always, feel free to reach out with feedback on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt;!&lt;/p&gt;</content><author><name>Micah</name></author><summary type="html">The papers over the next few weeks will be from SOSP. As always, feel free to reach out on Twitter with feedback or suggestions about papers to read! These paper reviews can be delivered weekly to your inbox, or you can subscribe to the Atom feed.</summary></entry><entry><title type="html">RAMP-TAO: Layering Atomic Transactions on Facebook’s Online TAO Data Store</title><link href="http://www.micahlerner.com/2021/10/23/ramp-tao-layering-atomic-transactions-on-facebooks-online-tao-data-store.html" rel="alternate" type="text/html" title="RAMP-TAO: Layering Atomic Transactions on Facebook’s Online TAO Data Store" /><published>2021-10-23T00:00:00-07:00</published><updated>2021-10-23T00:00:00-07:00</updated><id>http://www.micahlerner.com/2021/10/23/ramp-tao-layering-atomic-transactions-on-facebooks-online-tao-data-store</id><content type="html" xml:base="http://www.micahlerner.com/2021/10/23/ramp-tao-layering-atomic-transactions-on-facebooks-online-tao-data-store.html">&lt;p&gt;&lt;em&gt;The papers over the next few weeks will be from &lt;a href=&quot;https://sosp2021.mpi-sws.org/&quot;&gt;SOSP&lt;/a&gt;, which is taking place October 26-29th, 2021. As always, feel free to reach out on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt; with feedback or suggestions about papers to read! These paper reviews can &lt;a href=&quot;https://www.getrevue.co/profile/systems-weekly&quot;&gt;be delivered weekly to your inbox&lt;/a&gt;, or you can subscribe to the &lt;a href=&quot;https://www.micahlerner.com/feed.xml&quot;&gt;Atom feed&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/papers/ramp-tao.pdf&quot;&gt;RAMP-TAO: Layering Atomic Transactions on Facebook’s Online TAO Data Store&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is the second in a two part series on TAO, Facebook’s eventually-consistent&lt;label for=&quot;ec&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;ec&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;I like &lt;a href=&quot;https://www.allthingsdistributed.com/2008/12/eventually_consistent.html&quot;&gt;this description&lt;/a&gt; of what eventual consistency means from Werner Vogels, Amazon’s CTO. &lt;/span&gt; graph datastore. The &lt;a href=&quot;/2021/10/13/tao-facebooks-distributed-data-store-for-the-social-graph.html&quot;&gt;first part&lt;/a&gt; provides background on the system. This part (the second in the series) focuses on TAO-related research published at this year’s VLDB - &lt;a href=&quot;https://www.vldb.org/pvldb/vol14/p3014-cheng.pdf&quot;&gt;RAMP-TAO: Layering Atomic Transactions on Facebook’s Online TAO Data Store&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The paper on RAMP-TAO describes the design and implementation of transactional semantics on top of the existing large scale distributed system, which “serves over ten billion reads and tens of millions of writes per second on a changing data set of many petabytes”. This work is motivated by the difficulties that a lack of transactions poses for both internal application developers and external users.&lt;/p&gt;

&lt;p&gt;Adding transactional semantics to the existing system was made more difficult by other external engineering requirements - applications should be able gradually migrate to the new functionality and any new approach should have limited impact on the performance of existing applications. In building their solution, the authors adapt an existing protocol, called &lt;em&gt;RAMP&lt;/em&gt;&lt;label for=&quot;ramp&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;ramp&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;While I give some background on RAMP further on in this paper review, &lt;a href=&quot;http://www.bailis.org/blog/scalable-atomic-visibility-with-ramp-transactions/&quot;&gt;Peter Bailis&lt;/a&gt; (an author on the RAMP and RAMP-TAO papers) and &lt;a href=&quot;https://blog.acolyer.org/2015/03/27/scalable-atomic-visibility-with-ramp-transactions/&quot;&gt;The Morning Paper&lt;/a&gt; both have great overviews. &lt;/span&gt;, to TAO’s unique needs.&lt;/p&gt;

&lt;h2 id=&quot;tao-background&quot;&gt;TAO Background&lt;/h2&gt;

&lt;p&gt;This section provides a brief background on TAO - feel free to skip to the next section if you have either read &lt;a href=&quot;/2021/10/13/tao-facebooks-distributed-data-store-for-the-social-graph.html&quot;&gt;last week’s paper review&lt;/a&gt;, or the original TAO paper is fresh in your mind. TAO is an eventually consistent datastore that represents Facebook’s graph data using two database models - associations (edges) and objects (nodes).&lt;/p&gt;

&lt;p&gt;To respond to the read-heavy demands placed on the system, the infrastructure is divided into two layers - the storage layer (MySQL databases which store the backing data) and the cache layer (which stores query results). The data in the storage layer is divided into many &lt;em&gt;shards&lt;/em&gt;, and there are many copies of any given shard. Shards are kept in sync with leader/follower replication.&lt;/p&gt;

&lt;p&gt;Reads are first sent to the cache layer, which aims to serve as many queries as possible via cache hits. On a cache miss, the cache is updated with data from the storage layer. Writes are forwarded to the leader for a shard, and eventually replicated to followers - as seen in &lt;a href=&quot;https://research.fb.com/publications/wormhole-reliable-pub-sub-to-support-geo-replicated-internet-services/&quot;&gt;other papers&lt;/a&gt;, Facebook invests significant engineering effort into the technology that handles this replication with low latency and high availability.&lt;/p&gt;

&lt;h2 id=&quot;what-are-the-papers-contributions&quot;&gt;What are the paper’s contributions?&lt;/h2&gt;

&lt;p&gt;The RAMP-TAO paper makes four main contributions. It explains the need for transactional semantics in TAO, quantifies the problem’s impact, provides an implementation that fits the unique engineering constraints (which are covered in future sections), and demonstrates the feasability of the implementation with benchmarks.&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;The paper begins by discussing why transactional semantics matter in TAO, then provides examples of how application developers have worked around their omission from the original design.&lt;/p&gt;

&lt;h3 id=&quot;example-problems&quot;&gt;Example problems&lt;/h3&gt;

&lt;p&gt;The lack of transactional semantics in TAO allows two types of problems to crop up: &lt;em&gt;partially successful writes&lt;/em&gt; and &lt;em&gt;fractured reads&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;If writes are not batched together in transactions, it is possible for some of them to succeed and others to fail (&lt;em&gt;partially successful writes&lt;/em&gt;), resulting in an incorrect state of the system (as evidenced by the figure below).&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/ramp-tao/partially-successful-writes.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;A &lt;em&gt;fractured read&lt;/em&gt; is “a read result that captures partial transactional updates”, causing an inconsistent state to be returned to an application. &lt;em&gt;Fractured reads&lt;/em&gt; happen because of a combination of TAO’s eventual consistency and lack of transactional semantics - writes to different shards are replicated independently. Eventually all of the writes will be reflected in a copy of the dataset receiving these updates. In the meantime, it is possible for only some of the writes to be reflected in the dataset.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/ramp-tao/fractured-read.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;To address these two problems, the authors aruge that TAO must fulfill two guarantees:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Failure atomicity&lt;/em&gt; addresses &lt;em&gt;partially successful writes&lt;/em&gt; by ensuring “either all or none of the items in a write transaction are persisted.”&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Atomic visibility&lt;/em&gt; addresses &lt;em&gt;fractured reads&lt;/em&gt; by ensuring “a property that guarantees that either all or none of any transaction’s updates are visible to other transactions.”&lt;label for=&quot;stale&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;stale&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;As we will see later on in the paper review, it is preferable that TAO serves stale (rather than incorrect) data. &lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;existing-failure-atomicity-solutions-in-tao&quot;&gt;Existing failure atomicity solutions in TAO&lt;/h3&gt;

&lt;p&gt;The paper notes three existing approaches used to address &lt;em&gt;failure atomicity&lt;/em&gt; for applications built on TAO:  &lt;em&gt;single-shard MultiWrites&lt;/em&gt;, &lt;em&gt;cross-shard transactions&lt;/em&gt;, and &lt;em&gt;background repair&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Single-shard MultiWrites&lt;/em&gt; allows an application to perform many writes to the same shard (each shard of the data in TAO is stored as an individual database), meaning that this approach is able to use “MySQL transactions and their ACID properties” to ensure that all writes succeed or none of them do. There are several downsides including (but not limited to) hotspotting&lt;label for=&quot;hotspotting&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;hotspotting&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;If an application uses this approach, it will send many writes to a single machine/shard, which also could cause the shard to be larger than it would be otherwise. &lt;/span&gt; and the requirement that applications structure their schema/code to leverage the approach&lt;label for=&quot;migrating&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;migrating&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;If a paper isn’t architected with this approach in mind, the paper notes that migrating an already-deployed application to use &lt;em&gt;single-shard MultiWrites&lt;/em&gt; at scale is difficult. &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cross-shard transactions&lt;/em&gt; allow writes to be executed across multiple shards using a two-phase commit protocol (a.k.a 2PC)&lt;label for=&quot;2pc&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;2pc&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;For more on 2PC, I highly recommend &lt;a href=&quot;https://www.the-paper-trail.org/post/2008-11-27-consensus-protocols-two-phase-commit/&quot;&gt;this article&lt;/a&gt; from &lt;a href=&quot;https://twitter.com/henryr&quot;&gt;Henry Robinson&lt;/a&gt;. &lt;/span&gt; to roll back or restart transactions as needed. While this approach ensures that writes are &lt;em&gt;failure atomic&lt;/em&gt; (all writes succeed or none of them do), it does not provide &lt;em&gt;atomic visibility&lt;/em&gt; (“all of a transactions updates are visible or none of them are”), as the writes from a stalled transaction will be partially visible.&lt;/p&gt;

&lt;p&gt;The last approach is &lt;em&gt;background repair&lt;/em&gt;. Certain entities in the database, like edges for which there will always be a complement (called bidirectional associations), can be automatically checked to ensure that both edges exist. Unfortunately, this technique is limited to a subset of all of the entities stored in TAO, as this property is not universal.&lt;/p&gt;

&lt;h2 id=&quot;measuring-failure&quot;&gt;Measuring failure&lt;/h2&gt;

&lt;p&gt;To determine the engineering requirements facing an implementation of transactional semantics in TAO, the paper evaluates how frequently and for how long &lt;em&gt;fractured reads&lt;/em&gt; persist. The paper doesn’t dig as much into quantifying write-failures - while &lt;em&gt;failure atomicity&lt;/em&gt; is a property that the system should have, &lt;em&gt;cross-shard transactions&lt;/em&gt; roughly fill the requirement. Even so, &lt;em&gt;cross-shard transactions&lt;/em&gt; are still susceptible to &lt;em&gt;atomic visibility&lt;/em&gt; violations where some (but not all) of the writes from an in-progress transaction are visible to applications using TAO.&lt;/p&gt;

&lt;p&gt;The results from the measurement study indicate that 1 in 1,500 transactions violate &lt;em&gt;atomic visibility&lt;/em&gt;, noting that:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;45% of these fractured reads last for only a short period of time (i.e., naïvely retrying within a few seconds resolves these anomalies). After a closer look, these short-lasting anomalies occur when read and write transactions begin within 500 ms of each other. For these atomic visibility violations, their corresponding write transactions were all successful.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For the rest of the violations (those that are not fixed within 500ms):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;these atomic visibility violations could not be fixed within a short retry window and last up to 13 seconds. For this set of anomalies, their overlapping write transactions needed to undergo the 2PC failure recovery process, during which read anomalies persisted.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The paper’s authors argue that atomic visibility violations pose difficulties for engineers building applications with TAO, as “any decrease in write availability (e.g., from service deployment, data center maintenance, to outages) increases the probability that write transactions will stall, leading in turn to more read anomalies”.&lt;/p&gt;

&lt;h2 id=&quot;design&quot;&gt;Design&lt;/h2&gt;

&lt;p&gt;Following the measurement study, the paper pivots to discussing the design of a read API that provides &lt;em&gt;atomic visibility&lt;/em&gt; for TAO - there are three components to the design:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Choosing an isolation model&lt;label for=&quot;isolation model&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;isolation model&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Isolation models define how transactions observe the impact of other running/completed transactions - related blog post from FaunaDB &lt;a href=&quot;https://fauna.com/blog/introduction-to-transaction-isolation-levels&quot;&gt;here&lt;/a&gt;. This page from &lt;a href=&quot;https://jepsen.io/consistency&quot;&gt;Jepsen&lt;/a&gt; discusses the different, but related topic of distributed system consistency models. &lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;Constraints posed by the existing TAO infrastructure.&lt;/li&gt;
  &lt;li&gt;The protocol that clients will use to eliminate &lt;em&gt;atomic visibility violations&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;isolation-model&quot;&gt;Isolation model&lt;/h3&gt;

&lt;p&gt;The paper considers whether a Snapshot Isolation, Read Atomic isolation, or Read Uncommitted isolation model best solve the requirement of eliminating &lt;em&gt;atomic visibility&lt;/em&gt; violations (while maintaining the performance of the existing read-heavy workloads served by TAO). The authors choose Read Atomic isolation as it does not introduce unncessary features at the cost of performance as Snapshot Isolation does&lt;label for=&quot;snapshot&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;snapshot&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Snapshot Isolation provides point-in-time snapshots of a database useful for analytical queries, which TAO is not focused on supporting. &lt;/span&gt;, nor does it allow fractured reads as Read Committed does&lt;label for=&quot;rc&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;rc&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Read Committed “prevents access to uncommitted or intermediate versions of data”, but it is possible for TAO transactions to be committed, but not replicated. &lt;/span&gt;.&lt;/p&gt;

&lt;h3 id=&quot;design-constraints&quot;&gt;Design constraints&lt;/h3&gt;

&lt;p&gt;To implement Read Atomic isolation, the authors turn to the RAMP protocol&lt;label for=&quot;ramp&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;ramp&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;While I give some background on RAMP, &lt;a href=&quot;http://www.bailis.org/blog/scalable-atomic-visibility-with-ramp-transactions/&quot;&gt;Peter Bailis&lt;/a&gt; (an author on the RAMP and RAMP-TAO papers) and &lt;a href=&quot;https://blog.acolyer.org/2015/03/27/scalable-atomic-visibility-with-ramp-transactions/&quot;&gt;The Morning Paper&lt;/a&gt; both have great overviews. &lt;/span&gt; (short for &lt;em&gt;Read Atomic Multiple Partition&lt;/em&gt;) - several key ideas in RAMP fit well within the paradigm that TAO uses (where there are multiple partitions of the data) and can achieve &lt;em&gt;Read Atomic&lt;/em&gt; isolation.&lt;/p&gt;

&lt;p&gt;The RAMP read protocol works in two phases:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In the first round, RAMP sends out read requests for all data items and detects nonatomic reads&lt;label for=&quot;nonatomic&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;nonatomic&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Which could happen if only part of another transaction’s writes were visible. &lt;/span&gt;. In the second round, the algorithm explicitly repairs these reads by fetching any missing versions. RAMP writers use a modified two-phase commit protocol that requires metadata to be attached to each update, similar to the mechanism used by cross-shard write transactions on TAO.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Unfortunately, the original RAMP implementation can not be directly implemented in TAO, as the original paper operates with different assumptions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;RAMP assumes that all transactions in the system are using the protocol, but it is infeasible to have all TAO clients support the new functionality on day one. In the meantime, unupgraded clients shouldn’t incur the protocol’s overhead.&lt;/li&gt;
  &lt;li&gt;RAMP maintains metadata for each item, but doesn’t consider replicating that data to increase availability&lt;label for=&quot;metadata&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;metadata&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;There are many replicas of each shard in TAO, so the metadata has to be copied for every shard. &lt;/span&gt;, like TAO will need to.&lt;/li&gt;
  &lt;li&gt;RAMP assumes multiple versions of data is available, although this is not true - TAO maintains a single version for each row.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While the solutions to the first two challenges are non-trivial, they are relatively more straightforward - the first is addressed by gradually rolling out the functionality to applications, while the problem of metadata size is solved by applying specific structuring to MySQL tables. The next section of this paper review focuses on how TAO addresses the third challenge of “multiversioning”.&lt;/p&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;RAMP-TAO adapts the existing RAMP&lt;label for=&quot;ramp&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;ramp&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Specifically, the paper adapts one of three RAMP variants, RAMP-FAST. Each RAMP variant TODO &lt;/span&gt; protocol to fit the specifics of Facebook’s use case. This section describes a critical piece of Facebook infrastructure (called the &lt;em&gt;RefillLibrary&lt;/em&gt;) used in TAO’s implementation, as well as how RAMP-TAO works.&lt;/p&gt;

&lt;h3 id=&quot;the-refilllibrary&quot;&gt;The RefillLibrary&lt;/h3&gt;

&lt;p&gt;First, RAMP-TAO uses an existing piece of Facebook infrastructure called the &lt;em&gt;RefillLibrary&lt;/em&gt; to add support for “limited multiversioning” - “the RefillLibrary is a metadata buffer recording recent writes within TAO, and it stores approximately 3 minutes of writes from all regions”. By including additional metadata about whether items in the buffer were impacted by write transactions, RAMP-TAO can ensure that the system doesn’t violate &lt;em&gt;atomic visibility&lt;/em&gt;.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/ramp-tao/refill.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;When a read happens, TAO first checks whether the items being read are in the &lt;em&gt;RefillLibrary&lt;/em&gt;. If any items are in the &lt;em&gt;RefillLibrary&lt;/em&gt; and are marked as being written in a transaction, TAO returns metadata about the write to the caller. The caller in turn uses this metadata to perform logic that ensure &lt;em&gt;atomic visibility&lt;/em&gt; (described in the next section). If there is not a corresponding element in the &lt;em&gt;RefillLibrary&lt;/em&gt; for an item, “there are two possibilities: either it has been evicted (aged out) or it was updated too recently and has not been replicated to the local cache.”&lt;/p&gt;

&lt;p&gt;To determine which situation applies, TAO compares the timestamp of the oldest item in the &lt;em&gt;RefillLibrary&lt;/em&gt; to the timestamps of the items being read.&lt;/p&gt;

&lt;p&gt;If the timestamps for all read items are older than the oldest timestamp in the &lt;em&gt;RefillLibrary&lt;/em&gt;, it is safe to assume replication is complete - writes are evicted after 3 minutes, and based on the measurement study there are few replication issues that last that long. On the other hand, RAMP-TAO needs to perform additional work if timestamps from read items are greater than the oldest timestamp in the &lt;em&gt;RefillLibrary&lt;/em&gt; (in other words, still within the 3 minute range), and there are no entries in the &lt;em&gt;RefillLibrary&lt;/em&gt; for those items. This situation occurs if a write has not been replicated to the given location. To resolve this case, TAO performs a database request, and returns the most recent version stored in the database to the client (who may use the data to ensure &lt;em&gt;atomic visibility&lt;/em&gt;, as discussed in the next section).&lt;/p&gt;

&lt;h3 id=&quot;the-ramp-tao-protocol&quot;&gt;The RAMP-TAO Protocol&lt;/h3&gt;

&lt;p&gt;A primary goal of the RAMP-TAO protocol is ensuring &lt;em&gt;atomic visibility&lt;/em&gt; (“a property that guarantees that either all or none of any transaction’s updates are visible to other transactions”). At the same time, RAMP-TAO aims to offer comparable performance for existing applications that migrate to the new technology. Existing applications that don’t make use of transactional semantics parallelize requests to TAO and use whatever the database returns, even if the result reflects state from an in-progress transaction. In contrast, RAMP-TAO resolves situations where data from in-progress transactions is returned to applications.&lt;/p&gt;

&lt;p&gt;There are two primary paths that read requests in RAMP-TAO take: the &lt;em&gt;fast path&lt;/em&gt; and the &lt;em&gt;slow path&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;fast path&lt;/em&gt; happens in one round - the clients issue parallel read requests, and the returned data doesn’t reflect the partial result of an in-progress transaction&lt;label for=&quot;hooray&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;hooray&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Hooray! &lt;/span&gt;.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/ramp-tao/fast.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;In contrast, RAMP-TAO follows the &lt;em&gt;slow path&lt;/em&gt; when data is returned to the client that reflects an in-progress write transaction. In this situation, TAO reissues read requests to resolve the &lt;em&gt;atomic visibility violation&lt;/em&gt;. One way that violations are resolved on the slow path is by reissuing a request to fetch an older version of data - TAO applications are tolerant to serving stale, but correct, data.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/ramp-tao/slow.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;

&lt;p&gt;To evaluate the prototype system’s performance, the authors evaluate the performance of the protocol:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Our prototype serves over 99.93% of read transactions in one round of communication. Even when a subsequent round is necessary, the performance impact is small and bounded to under 114ms in the 99𝑡ℎ percentile (Figure 12). Our tail latency is within the range of TAO’s P99 read latency of 105ms for a similar workload. We note that these are the worst-case results for RAMP-TAO because the prototype currently requires multiple round trips to the database for transaction metadata. Once the changes to the RefillLibrary are in place, the large majority of the read transactions can be directly served with data in this buffer and will take no longer than a typical TAO read.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;While RAMP-TAO is still in development (and will require further changes to both applications and Facebook infrastructure), it is exciting to see the adaptation of existing systems to different constraints - unlike systems built from scratch, RAMP-TAO also needed to balance unique technical considerations like permitting gradual adoption. I enjoyed the RAMP-TAO paper as it not only solves a difficult technical problem, but also clearly outlines the thinking and tradeoffs behind the design.&lt;/p&gt;

&lt;p&gt;As always, feel free to reach out with feedback on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt;!&lt;/p&gt;</content><author><name>Micah</name></author><summary type="html">The papers over the next few weeks will be from SOSP, which is taking place October 26-29th, 2021. As always, feel free to reach out on Twitter with feedback or suggestions about papers to read! These paper reviews can be delivered weekly to your inbox, or you can subscribe to the Atom feed.</summary></entry><entry><title type="html">TAO: Facebook’s Distributed Data Store for the Social Graph</title><link href="http://www.micahlerner.com/2021/10/13/tao-facebooks-distributed-data-store-for-the-social-graph.html" rel="alternate" type="text/html" title="TAO: Facebook’s Distributed Data Store for the Social Graph" /><published>2021-10-13T00:00:00-07:00</published><updated>2021-10-13T00:00:00-07:00</updated><id>http://www.micahlerner.com/2021/10/13/tao-facebooks-distributed-data-store-for-the-social-graph</id><content type="html" xml:base="http://www.micahlerner.com/2021/10/13/tao-facebooks-distributed-data-store-for-the-social-graph.html">&lt;p&gt;&lt;em&gt;The papers over the next few weeks will be from (or related to) research from &lt;a href=&quot;https://vldb.org/2021/?info-research-papers&quot;&gt;VLDB 2021&lt;/a&gt; - on the horizon is one of my favorite systems conferences &lt;a href=&quot;https://sosp2021.mpi-sws.org/&quot;&gt;SOSP&lt;/a&gt;. As always, feel free to reach out on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt; with feedback or suggestions about papers to read! These paper reviews can &lt;a href=&quot;https://newsletter.micahlerner.com/&quot;&gt;be delivered weekly to your inbox&lt;/a&gt;, or you can subscribe to the &lt;a href=&quot;https://www.micahlerner.com/feed.xml&quot;&gt;Atom feed&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.usenix.org/system/files/conference/atc13/atc13-bronson.pdf&quot;&gt;TAO: Facebook’s Distributed Data Store for the Social Graph&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is the first in a two part series on TAO&lt;label for=&quot;tao&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;tao&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;TAO stands for “The Associations and Objects” - associations are the edges in graph, and objects are the nodes. &lt;/span&gt;, Facebook’s read-optimized, eventually-consistent graph database. Unlike other graph databases, TAO focuses exclusively on serving and caching a constrained set of application requests at scale (in contrast to systems focused on data analysis). Furthermore, the system builds on expertise scaling MySQL and memcache as discussed in a &lt;a href=&quot;https://www.micahlerner.com/2021/05/31/scaling-memcache-at-facebook.html&quot;&gt;previous paper review&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The first paper in the series focuses on the original TAO paper, describing the motivation for building the system, it’s architecture, and engineering lessons learned along the way. The second part focuses on TAO-related research published at this year’s VLDB - &lt;a href=&quot;https://www.vldb.org/pvldb/vol14/p3014-cheng.pdf&quot;&gt;RAMP-TAO: Layering Atomic Transactions on Facebook’s Online TAO Data Store&lt;/a&gt;. This new paper describes the design and implementation of transactions on top of the existing large scale distributed system - a task made more difficult by the requirement that applications should gradually migrate to the new functionality and that the work to support transactions should have limited impact on the performance of existing applications.&lt;/p&gt;

&lt;h2 id=&quot;what-are-the-papers-contributions&quot;&gt;What are the paper’s contributions?&lt;/h2&gt;

&lt;p&gt;The original TAO paper makes three contributions - characterizing and motivating a graph database implementation suited for Facebook’s read-heavy traffic, providing a data model and developer API for the aforementioned database, and describing the architecture that allowed the database to scale.&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;The paper begins with a section describing the motivation for TAO’s initial development. When Facebook was originally developed, MySQL was used as the datastore for the graph. As the site scaled, a memcache layer was added in front of the MySQL databases, to lighten the load.&lt;/p&gt;

&lt;p&gt;While inserting memcache into the stack worked for some period of time, the paper cites three main problems with the implementation: &lt;em&gt;inefficient edge lists&lt;/em&gt;, &lt;em&gt;distributed control logic&lt;/em&gt;, and &lt;em&gt;expensive read-after-write consistency&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;inefficient-edge-lists&quot;&gt;Inefficient edge lists&lt;/h3&gt;

&lt;p&gt;Application developers within Facebook used &lt;em&gt;edge-lists&lt;/em&gt; to represent aggregations of the information in the graph - for example, a list of the friendships a user has (each friendship is an edge in the graph, and the users are the nodes). Unfortunately, maintaining these lists in memcache was inefficient - memcache is a simple key value store without support for lists, meaning that common list-related functionality&lt;label for=&quot;redis&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;redis&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Like that supported in &lt;a href=&quot;https://redis.io/topics/data-types#lists&quot;&gt;Redis&lt;/a&gt;. &lt;/span&gt;  is inefficient. If a list needs to be updated (say for example, a friendship is deleted), the logic to update the list would be complicated - in particular, the part of the logic related to coordinating the update of the list across several copies of the same data in multiple data centers.&lt;/p&gt;

&lt;h3 id=&quot;distributed-control-logic&quot;&gt;Distributed Control Logic&lt;/h3&gt;

&lt;p&gt;Control logic (in the context of Facebook’s graph store architecture) means the ability to manipulate how the system is accessed. Before TAO was implemented, the graph data store had &lt;em&gt;distributed control logic&lt;/em&gt; - clients communciated directly with the memcache nodes, and there is not a single point of control to gate client access to the system. This property makes it difficult to guard against misbehaving clients and &lt;a href=&quot;https://instagram-engineering.com/thundering-herds-promises-82191c8af57d&quot;&gt;thundering herds&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;expensive-read-after-write-consistency&quot;&gt;Expensive read-after-write consistency&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Read-after-write consistency&lt;/em&gt; means that if a client writes data, then performs a read of the data, the client should see the result of the write that it performed. If a system doesn’t have this property, users might be confused - “why did the like button they just pressed not register when they reloaded the page?”.&lt;/p&gt;

&lt;p&gt;Ensuring read-after-write consistency was expensive and difficult for Facebook’s memcache-based system, which used MySQL databases with master/slave replication to propagate database writes between datacenters. While Facebook developed internal technology&lt;label for=&quot;memcache&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;memcache&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;As described in my previous paper review, &lt;a href=&quot;https://www.micahlerner.com/2021/05/31/scaling-memcache-at-facebook.html&quot;&gt;Scaling Memcache at Facebook&lt;/a&gt;. &lt;/span&gt; to stream changes between databases, existing systems that used the MySQL and memcache combo relied on complicated cache-invalidation logic&lt;label for=&quot;cacheinvalid&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;cacheinvalid&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;For example, followers would forward reads for cache keys invalidated by a write to the leader database, increasing load and incurring potentially slow inter-regional communication. &lt;/span&gt; that incurred networking overhead. The goal of this new system is to avoid this overhead (with an approach described later in the paper).&lt;/p&gt;

&lt;h2 id=&quot;data-model-and-api&quot;&gt;Data model and API&lt;/h2&gt;

&lt;p&gt;TAO is an eventually consistent&lt;label for=&quot;werner&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;werner&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;For a description of eventual consistency (and related topics!), I highly recommend &lt;a href=&quot;https://www.allthingsdistributed.com/2008/12/eventually_consistent.html&quot;&gt;this post&lt;/a&gt; from Werner Vogels. &lt;/span&gt; read-optimized data store for the Facebook graph.&lt;/p&gt;

&lt;p&gt;It stores two entities - &lt;em&gt;objects&lt;/em&gt; and &lt;em&gt;associations&lt;/em&gt; (the relationships between objects). Now we get to learn why the graph datastore is called TAO - the name is an abbreviation that stands for “The Associations and Objects”.&lt;/p&gt;

&lt;p&gt;As an example of how &lt;em&gt;objects&lt;/em&gt; and &lt;em&gt;associations&lt;/em&gt; are used to model data, consider two common social network functions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Friendships between users&lt;/em&gt;: users in the database are stored as &lt;em&gt;objects&lt;/em&gt;, and the relationship between users are &lt;em&gt;associations&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Check-ins&lt;/em&gt;: the user and the location they check in to are &lt;em&gt;objects&lt;/em&gt;. An &lt;em&gt;association&lt;/em&gt; exists between them to represent that the given user has checked into a given location.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Objects and associations have different database representations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Each &lt;em&gt;object&lt;/em&gt; in the database has an id and type.&lt;/li&gt;
  &lt;li&gt;Each &lt;em&gt;association&lt;/em&gt; contains the ids of the objects connected by the given edge, as well as the type of the association (check-in, friendship, etc). Additionally, each association has a timestamp that is used for querying (described later in the paper review).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Key-value metadata can be associated with both objects and associations, although the possible keys, and value type are constrained by the type of the object or association.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/tao-pt1/objects.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;To provide access to this data, TAO provides three main APIs: the &lt;em&gt;Object API&lt;/em&gt;, the &lt;em&gt;Association API&lt;/em&gt;, and the &lt;em&gt;Association Querying API&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Two of the three (the &lt;em&gt;Object API&lt;/em&gt; and &lt;em&gt;Association API&lt;/em&gt;) provide create, read, update, and delete operations for individual objects.&lt;/p&gt;

&lt;p&gt;In contrast, the &lt;em&gt;Association Querying API&lt;/em&gt; provides an interface for performing common queries on the graph. The query methods allow application developers to fetch associations for a given object and type (potentially constraining by time range or the set of objects that the the association points), calculating the count of associations for an object, and providing pagination-like functionality. The paper provides example query patterns like fetching the “50 most recent comments on Alice’s checkin”  or “how many checkins at the GG Bridge?”. Queries in this API return multiple associations, and call this type of result an &lt;em&gt;association list&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;p&gt;The architecture of TAO contains two layers, the &lt;em&gt;storage layer&lt;/em&gt; and the &lt;em&gt;caching layer&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;storage-layer&quot;&gt;Storage Layer&lt;/h3&gt;

&lt;p&gt;The &lt;em&gt;storage layer&lt;/em&gt; (as the name suggests) persists graph data in MySQL&lt;label for=&quot;mysql&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;mysql&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Facebook has invested a significant amount of resources in their MySQL deployments, as evidenced by their &lt;a href=&quot;https://engineering.fb.com/2016/08/31/core-data/myrocks-a-space-and-write-optimized-mysql-database/&quot;&gt;MyRocks&lt;/a&gt; storage engine and &lt;a href=&quot;https://engineering.fb.com/2021/07/22/data-infrastructure/mysql/&quot;&gt;other posts&lt;/a&gt; on their tech blog. &lt;/span&gt;. There are two key technical points to the storage layer: &lt;em&gt;shards&lt;/em&gt; and the &lt;em&gt;tables&lt;/em&gt; used to store the graph data itself.&lt;/p&gt;

&lt;p&gt;The graph data is divided into &lt;em&gt;shards&lt;/em&gt; (represented as a MySQL database), and shards are mapped to one of many database servers. Objects and associations for each shard are stored in separate tables.&lt;/p&gt;

&lt;h3 id=&quot;cache-layer&quot;&gt;Cache Layer&lt;/h3&gt;

&lt;p&gt;The cache layer is optimized for read requests and stores query results in memory. There are three key ideas in the cache layer: &lt;em&gt;cache servers&lt;/em&gt;, &lt;em&gt;cache tiers&lt;/em&gt;, and &lt;em&gt;leader/follower tiers&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Clients communicate read and write requests to &lt;em&gt;cache servers&lt;/em&gt;. Each &lt;em&gt;cache server&lt;/em&gt; services requests for a set of shards in the &lt;em&gt;storage layer&lt;/em&gt;, and caches objects, associatons, and the size of association lists (via the query patterns mentioned in the API section above).&lt;/p&gt;

&lt;p&gt;A &lt;em&gt;cache tier&lt;/em&gt; is a collection of &lt;em&gt;cache servers&lt;/em&gt; that can respond to requests for all shards - the number of cache servers in each tier is configurable, as is the mapping from request to cache server.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cache tiers&lt;/em&gt; can be set up as &lt;em&gt;leaders&lt;/em&gt; or &lt;em&gt;followers&lt;/em&gt;. Whether a cache tier is a &lt;em&gt;leader&lt;/em&gt; or a &lt;em&gt;follower&lt;/em&gt; impacts its behavior:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Follower tiers&lt;/em&gt; can serve read requests without communicating with the leader (although they forward read misses and write requests to the corresponding cache servers in the leader tier).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Leader tiers&lt;/em&gt; communicate with the storage layer (by reading and writing to/from the database), as well as with &lt;em&gt;follower&lt;/em&gt; cache tiers. In the background, the &lt;em&gt;leader tier&lt;/em&gt; sends cache update messages to &lt;em&gt;follower tiers&lt;/em&gt; (resulting in the eventual consistency mentioned earlier on in this paper review).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;scaling&quot;&gt;Scaling&lt;/h2&gt;

&lt;p&gt;To operate at large scale, TAO needed to extend beyond a single region. The system accomplishes this goal by using a master/slave configuration for each shard of the database.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/tao-pt1/ms.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;In the master/slave configuration, each shard has a single &lt;em&gt;leader&lt;/em&gt; cache tier and many &lt;em&gt;follower&lt;/em&gt; cache tiers. The data in the storage layer for each shard is replicated from the master region to slave regions asynchronously.&lt;/p&gt;

&lt;p&gt;A primary difference between the single region configuration described above and the multi-region configuration is the behavior of the &lt;em&gt;leader tier&lt;/em&gt; when it receives writes. In a single-region configuration, the leader tier always forwards writes to the &lt;em&gt;storage layer&lt;/em&gt;. In contrast, the leader tier in a multi-region TAO configuration writes to the &lt;em&gt;storage layer&lt;/em&gt; only if the &lt;em&gt;leader tier&lt;/em&gt; is in the &lt;em&gt;master region&lt;/em&gt;. If the &lt;em&gt;leader tier&lt;/em&gt; is not in the &lt;em&gt;master region&lt;/em&gt; (meaning it is in a slave region!), then the &lt;em&gt;leader tier&lt;/em&gt; needs to forward the write to the &lt;em&gt;master region&lt;/em&gt;. Once the &lt;em&gt;master region&lt;/em&gt; acknowledges the write, the slave region updates its local cache with the result of the write.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;TAO is a graph database operating at immense scale. The system was built on the emerging needs of Facebook, and had limited support for transactions&lt;label for=&quot;txns&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;txns&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The paper mentions limited transaction-like behavior but does not provide significant details &lt;/span&gt;. The next paper in the series discusses how transactions were added to the system, while maintaining performance for existing applications and providing an opt-in upgrade path for new applications.&lt;/p&gt;

&lt;p&gt;As always, feel free to reach out on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt; with any feedback or paper suggestions. Until next time&lt;/p&gt;</content><author><name>Micah</name></author><summary type="html">The papers over the next few weeks will be from (or related to) research from VLDB 2021 - on the horizon is one of my favorite systems conferences SOSP. As always, feel free to reach out on Twitter with feedback or suggestions about papers to read! These paper reviews can be delivered weekly to your inbox, or you can subscribe to the Atom feed.</summary></entry><entry><title type="html">Scaling Large Production Clusters with Partitioned Synchronization</title><link href="http://www.micahlerner.com/2021/10/10/scaling-large-production-clusters-with-partitioned-synchronization.html" rel="alternate" type="text/html" title="Scaling Large Production Clusters with Partitioned Synchronization" /><published>2021-10-10T00:00:00-07:00</published><updated>2021-10-10T00:00:00-07:00</updated><id>http://www.micahlerner.com/2021/10/10/scaling-large-production-clusters-with-partitioned-synchronization</id><content type="html" xml:base="http://www.micahlerner.com/2021/10/10/scaling-large-production-clusters-with-partitioned-synchronization.html">&lt;p&gt;&lt;em&gt;This is one of the last papers we will be reading from &lt;a href=&quot;https://www.usenix.org/conference/atc21&quot;&gt;Usenix ATC&lt;/a&gt; and &lt;a href=&quot;https://www.usenix.org/conference/osdi21&quot;&gt;OSDI&lt;/a&gt;. There are several great conferences coming up over the next few months that I’m excited to read through together. Next week we will be moving on to VLDB (Very Large Data Bases), and SOSP (Symposium on Operating Systems Principles) is coming up later this month. As always, feel free to reach out on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt; with feedback or suggestions about papers to read! These paper reviews can &lt;a href=&quot;https://newsletter.micahlerner.com/&quot;&gt;be delivered weekly to your inbox&lt;/a&gt;, or you can subscribe to the &lt;a href=&quot;https://www.micahlerner.com/feed.xml&quot;&gt;Atom feed&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p class=&quot;discussion&quot;&gt;Discussion on &lt;a href=&quot;https://news.ycombinator.com/item?id=28440542&quot;&gt; Hacker News&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.usenix.org/system/files/atc21-feng-yihui.pdf&quot;&gt;Scaling Large Production Clusters with Partitioned Synchronization&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This week’s paper review won a best paper award at Usenix ATC, and discusses Alibaba’s approach to scaling their production environment. In particular, the paper focuses on the evolution of the scheduling architecture used in Alibaba datacenters in response to growth in workloads and resources&lt;label for=&quot;scale&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;scale&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;An increase in resources or workloads impacted the load on the existing scheduler architecture. The former translates into more options for the scheduler to choose from when scheduling, and the latter means more computation that needs to be performed by the scheduler. &lt;/span&gt;. Beyond discussing Alibaba’s specific challenges and solutions, the paper also touches on the landscape of existing scheduler architectures (like Mesos, YARN, and Omega).&lt;/p&gt;

&lt;h2 id=&quot;scheduler-architectures&quot;&gt;Scheduler architectures&lt;/h2&gt;

&lt;p&gt;The paper first aims to decide whether any existing scheduling architectures meet the neeeds of Alibaba’s production environment - any solution to the scaling problem’s encountered by Alibaba’s system needed to not only scale, but also simultaneously provide backward compatibility for existing users of the cluster (who have invested significant engineering effort to ensure their workloads are compatible with existing infrastructure).&lt;/p&gt;

&lt;p&gt;To evaluate future scheduler implementations, the authors considered several requirements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Low scheduling delay&lt;/em&gt;: the selected scheduler should be capable of making decisions quickly.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;High scheduling quality&lt;/em&gt;: if a task specifies preferences for resources, like running on “machines where its data are stored” or “machines with larger memory or faster CPUs”, those preferences should be fulfilled as much as possible.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Fairness&lt;/em&gt;: tasks should be allocated resources according to their needs (without being allowed to hog them)&lt;label for=&quot;fairness&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;fairness&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;There are a number of interesting papers on fairness, like &lt;a href=&quot;https://cs.stanford.edu/~matei/papers/2011/nsdi_drf.pdf&quot;&gt;Dominant Resource Fairness: Fair Allocation of Multiple Resource Types&lt;/a&gt; (authored by founders of Spark and Mesos). &lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Resource utilization&lt;/em&gt;: the scheduler should aim to use as much of the cluster’s resources as possible.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These requirements are then applied to four existing scheduler architectures&lt;label for=&quot;omega&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;omega&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The &lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41684.pdf&quot;&gt;Omega paper&lt;/a&gt; is also an excellent resource on this topic, and the figure below is sourced from there &lt;/span&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Monolithic&lt;/em&gt;: an architecture with a single instance that lacks parallelism, common in HPC settings or lower-scale cloud environments.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Statically partitioned&lt;/em&gt;: generally used for fixed-size clusters that run dedicated jobs or workloads (like Hadoop).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Two-level&lt;/em&gt;: a scheduling strategy where a central cordinator assigns resources to sub-schedulers. This is implemented by &lt;a href=&quot;https://people.eecs.berkeley.edu/~alig/papers/mesos.pdf&quot;&gt;Mesos&lt;/a&gt;, which uses “frameworks” to schedule tasks on resources offered by the central scheduler. &lt;a href=&quot;http://mesos.apache.org/documentation/latest/frameworks/&quot;&gt;Examples of frameworks&lt;/a&gt; are batch schedulers, big data processing systems (like Spark), and service schedulers. A Mesos-like implementation is labeled “pessimistic concurrency control” because it aims to ensure that there will few (or no) conflicts between schedulers.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Shared-state&lt;/em&gt;: one or more schedulers read shared cluster metadata about resources, then use that metadata to make scheduling decisions.  To schedule tasks, the independent schedulers try to modify the shared state. Because multiple schedulers are reading from and attempting to write to the same state, modifications may conflict. In the event of a conflict, one scheduler succeeds and others fail (then re-evaluate their scheduling decisions). &lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41684.pdf&quot;&gt;Omega&lt;/a&gt; is a shared-state scheduler cited by the authors. An Omega-like implementation utilizes “optimistic concurrency control” because the design assumes that there will be few conflicts between schedulers (and only performs additional work to resolve conflicts when they actually happen).&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/parsync/schedulerarch.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;Scheduler architecture diagram sourced from the &lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41684.pdf&quot;&gt;Omega paper&lt;/a&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The authors decide, after applying their requirements to existing scheduler architectures, to extend the design of &lt;em&gt;Omega&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;After making this decision, the paper notes that a potential issue with an &lt;em&gt;Omega&lt;/em&gt;-based architecture at scale is &lt;em&gt;contention&lt;/em&gt;. &lt;em&gt;Contention&lt;/em&gt; occurs when multiple schedulers attempt to schedule tasks with the same resources - in this situation, one of the scheduling decisions succeeds, and all others could be rejected (meaning that the schedulers who issued the now-failed requests need to re-calculate scheduling decisions.&lt;/p&gt;

&lt;p&gt;The authors spend the majority of the paper evaluating how contention can be reduced, as it could pose a limit to the scalability of the future scheduler. In the process, the paper performs multiple simulations to evaluate the impact of adjusting critical scheduling-related variables.&lt;/p&gt;

&lt;h2 id=&quot;what-are-the-papers-contributions&quot;&gt;What are the paper’s contributions?&lt;/h2&gt;

&lt;p&gt;The paper makes three contributions. After outlining existing scheduler architectures, it evaluates (using simulation techniques) how the selected approach would handle possible contention if adopted in Alibaba’s production environment. Using these results, the paper suggests an extension to the shared-state scheduling architecture. Lastly, the paper characterizes the performance of this solution, and provides a framework for simulating its performance under a variety of loads.&lt;/p&gt;

&lt;h2 id=&quot;modeling-scheduling-conflicts&quot;&gt;Modeling scheduling conflicts&lt;/h2&gt;

&lt;p&gt;As mentioned above, more tasks competing for the same set of resources means &lt;em&gt;contention&lt;/em&gt; - jobs will try to schedule tasks to the same slots (“slots” in this context correspond to resources). Given the optimistic concurrency control approach taken in an &lt;em&gt;Omega&lt;/em&gt;-influenced shared-state scheduler, the paper argues that there will be latency introduced by scheduling conflicts.&lt;/p&gt;

&lt;p&gt;To evaluate potential factors that impact in a cluster under high load, the paper considers the effect of additional schedulers. Adding extra schedulers (while keeping load constant) spreads the load over more instances. Lower per-scheduler loads corresponds to lower delay in the event of contention&lt;label for=&quot;contention&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;contention&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;If a scheduling decision fails, the failed request doesn’t compete with a long queue of other requests. &lt;/span&gt;, although there are diminishing returns to flooding the cluster with schedulers&lt;label for=&quot;cost&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;cost&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Not to mention the cost of adding more schedulers - each scheduler likely has multiple backup schedulers running, ready to take over if the primary fails. &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;For each number of schedulers, the simulation varies:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Task Submission Rate&lt;/em&gt;: the number of decisions the cluster needs to make per unit time.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Synchronization Gap&lt;/em&gt;: how long a scheduler has in between refreshing its state of the cluster.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Variance of slot scores&lt;/em&gt;: the number of “high-quality” slots available in the system. This is a proxy for the fact that certain resource types in the cluster are generally more preferred in the cluster, leading to hotspots.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;The number of partitions of the master state&lt;/em&gt;: how many subdivisions of the master state there are (each part of the cluster’s resources would be assigned to a partition).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To evaluate the performance of different configurations, the experiment records the number of extra slots required to maintain a given scheduling delay. The count of additional slots is a proxy for actual performance. For example, if the task submission rate increases, one would expect that the number of extra slots required to maintain low scheduling delay would also increase. On the other hand, changing experimental variables (like the number of partitions of the master state) may not require more slots or schedulers.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/parsync/sim.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The experimental results indicate that flexibility in the system lies in the quality of the scheduling (&lt;em&gt;Variance of slot scores&lt;/em&gt;) and in the staleness of the local states (&lt;em&gt;Synchronization Gap&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;In other words, scheduling can scale by:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Relaxing constraints on scheduling decisions&lt;/em&gt;, possibly scheduling tasks to resources that are slower or don’t exactly fit the job’s needs).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Communicating more with nodes&lt;/em&gt; in order to get updated state about their resources: A scheduler that updates its state more frequently would have a more up-to-date view of the cluster (meaning that it would make fewer scheduling decisions that collide with recent operations by the other schedulers in the cluster). State syncing from cluster nodes to a centralized store is costly and grows with the number of nodes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;partitioned-synchronization&quot;&gt;Partitioned Synchronization&lt;/h2&gt;

&lt;p&gt;Up to date scheduler state leads to lower contention, but syncing the required state from nodes to achieve this goal is costly (both in networking traffic and space). To address this cost, the authors suggest an approach called &lt;em&gt;partitioned synchronization&lt;/em&gt; (a.k.a &lt;em&gt;ParSync&lt;/em&gt;) with the goal, “to reduce the staleness of the local states and to find a good balance between resource quality (i.e., slot score) and scheduling efficiency”. &lt;em&gt;ParSync&lt;/em&gt; works by syncing partitions of a cluster’s state to one of the many&lt;label for=&quot;parsync&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;parsync&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The previous section notes that there are significant performance benefits to adding schedulers in a shared-state architecture, up to a point. &lt;/span&gt; schedulers in a cluster. Then, the scheduling algorithm weights the recency (or &lt;em&gt;staleness&lt;/em&gt;) of a partition’s state in scheduling decisions.&lt;/p&gt;

&lt;p&gt;The authors argue that short-lived low latency tasks, as well as long-running batch jobs benefit from &lt;em&gt;ParSync&lt;/em&gt;. For example, if a task is short lived, it should be quickly scheduled - a non-ideal scheduler would take more time making decisions than the task takes to actually run. In this situation, &lt;em&gt;ParSync&lt;/em&gt;-based scheduling can assign the task to a recently updated partition, with high likelihood that the scheduling decision will succeed - other schedulers will not update the partition’s state at the same time, instead preferring their own recently updated partitions. On the other side of the spectrum, a long running job might prefer certain resources, trading off more time spent making a scheduling decision for running with preferred resources.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;ParSync&lt;/em&gt; is coupled with three scheduling strategies:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Quality-first&lt;/em&gt;: optimize for use of preferred resources.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Latency-first&lt;/em&gt;: optimize for faster scheduling decisions (even if they are non-optimal).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Adaptive&lt;/em&gt;: use the Quality-first or Latency-first strategy depending on whether scheduling delay is high or not. If there is low scheduling delay, the scheduler will prefer quality-first. If there is high scheduling delay, the scheduler prefers latency-first.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The next section discusses the performance of the three different strategies.&lt;/p&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;

&lt;p&gt;The paper results indicate that both quality-first and latency-first scheduling strategies don’t adapt to conditions they are not optimized for. Quality-first scheduling experiences latency at high load (when the scheduler should make decisions quickly), while latency-first scheduling generally makes worse scheduling decisions under low load (when the scheduler could take more time and choose ideal resources). In contrast, the adaptive strategy is able to switch between the aforementioned strategies, while achieving high resource utilization.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/parsync/eval.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This paper discusses a number of interesting scheduler architectures, as well as touching on the body of work covering scheduler internals&lt;label for=&quot;firmament&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;firmament&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;See &lt;a href=&quot;https://cs.stanford.edu/~matei/papers/2011/nsdi_drf.pdf&quot;&gt;Dominant Resource Fairness: Fair Allocation of Multiple Resource Types&lt;/a&gt; and &lt;a href=&quot;https://www.usenix.org/system/files/conference/osdi16/osdi16-gog.pdf&quot;&gt;Firmament: Fast, Centralized Cluster Scheduling at Scale&lt;/a&gt; &lt;/span&gt; (which I would love to read in the future). While the content of this paper leans heavily on simulation, there is a discussion of performance evaluation using internal Alibaba tools - I’m hopeful that we will be able to learn more about the real world performance of the team’s scheduler in future research (as we often see with industry papers).&lt;/p&gt;

&lt;p&gt;As always, feel free to reach out on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt; with any feedback or paper suggestions. Until next time!&lt;/p&gt;</content><author><name>Micah</name></author><summary type="html">This is one of the last papers we will be reading from Usenix ATC and OSDI. There are several great conferences coming up over the next few months that I’m excited to read through together. Next week we will be moving on to VLDB (Very Large Data Bases), and SOSP (Symposium on Operating Systems Principles) is coming up later this month. As always, feel free to reach out on Twitter with feedback or suggestions about papers to read! These paper reviews can be delivered weekly to your inbox, or you can subscribe to the Atom feed.</summary></entry><entry><title type="html">A Linux Kernel Implementation of the Homa Transport Protocol, Part II</title><link href="http://www.micahlerner.com/2021/08/29/a-linux-kernel-implementation-of-the-homa-transport-protocol.html" rel="alternate" type="text/html" title="A Linux Kernel Implementation of the Homa Transport Protocol, Part II" /><published>2021-08-29T00:00:00-07:00</published><updated>2021-08-29T00:00:00-07:00</updated><id>http://www.micahlerner.com/2021/08/29/a-linux-kernel-implementation-of-the-homa-transport-protocol</id><content type="html" xml:base="http://www.micahlerner.com/2021/08/29/a-linux-kernel-implementation-of-the-homa-transport-protocol.html">&lt;p&gt;&lt;em&gt;Programming note: I will be taking a several week break from writing paper reviews for the summer. When we come back, I will be finishing off the papers from &lt;a href=&quot;https://www.usenix.org/conference/atc21&quot;&gt;Usenix ATC&lt;/a&gt; and &lt;a href=&quot;https://www.usenix.org/conference/osdi21&quot;&gt;OSDI&lt;/a&gt;, then moving on to the great upcoming conferences (my non-exhaustive list is &lt;a href=&quot;https://www.micahlerner.com/2021/08/14/systems-conferences-2021.html&quot;&gt;here&lt;/a&gt;). As always, feel free to reach out on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt; with feedback or suggestions about papers to read! These paper reviews can &lt;a href=&quot;https://newsletter.micahlerner.com/&quot;&gt;be delivered weekly to your inbox&lt;/a&gt;, or you can subscribe to the new &lt;a href=&quot;https://www.micahlerner.com/feed.xml&quot;&gt;Atom feed&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.usenix.org/system/files/atc21-ousterhout.pdf&quot;&gt;A Linux Kernel Implementation of the Homa Transport Protocol&lt;/a&gt;&lt;/p&gt;

&lt;p class=&quot;discussion&quot;&gt;Discussion on &lt;a href=&quot;https://news.ycombinator.com/item?id=28440542&quot;&gt; Hacker News&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This week’s paper review is Part II in a series on the Homa Transport Protocol - part I is available &lt;a href=&quot;/2021/08/15/a-linux-kernel-implementation-of-the-homa-transport-protocol.html&quot;&gt;here&lt;/a&gt;. As a refresher, Homa is a transport protocol with the goal of replacing TCP in the data center. The first part of the series focuses on describing the goals of Homa, while this paper review discusses an &lt;a href=&quot;https://github.com/PlatformLab/HomaModule&quot;&gt;open source implementation&lt;/a&gt; of the protocol as a Linux Kernel module&lt;label for=&quot;linuxkernelmodule&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;linuxkernelmodule&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;There is an excellent Kernel Module programming &lt;a href=&quot;https://sysprog21.github.io/lkmpg/&quot;&gt;guide&lt;/a&gt; that has been revamped continuously since the 2.2 kernel. Another great description of writing your own Linux Kernel module &lt;a href=&quot;https://linux-kernel-labs.github.io/refs/heads/master/labs/kernel_modules.html&quot;&gt;here&lt;/a&gt;. &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;The author (John Ousterhout, one of the inventors of the &lt;a href=&quot;https://raft.github.io/&quot;&gt;Raft&lt;/a&gt; consensus algorithm) has three goals in mind with implementing Homa as a Linux Kernel Module:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Understand how Homa performs in a more production-like environment, represented by the Linux kernel.&lt;/li&gt;
  &lt;li&gt;Perform apples to apples comparisons of Homa to implementations of competing protocols (TCP and DCTCP).&lt;/li&gt;
  &lt;li&gt;Build an implementation of Homa that could be used and extended by real users&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-are-the-papers-contributions&quot;&gt;What are the paper’s contributions?&lt;/h2&gt;

&lt;p&gt;In accomplishing the three goals above, the paper makes two contributions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Showing that Homa beats TCP and DCTCP&lt;label for=&quot;dctcp&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;dctcp&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;DCTCP is a project that preceded Homa, but has a similar goal of replacing TCP in the dataceter. The paper is &lt;a href=&quot;https://people.csail.mit.edu/alizadeh/papers/dctcp-sigcomm10.pdf&quot;&gt;here&lt;/a&gt;. &lt;/span&gt;, replicating the results of the paper presented in &lt;a href=&quot;/2021/08/15/a-linux-kernel-implementation-of-the-homa-transport-protocol.html&quot;&gt;Part I&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;An analysis of Homa’s limits. This study indicates potential future directions for research tackling the tension between faster network speeds and using more cores to handle increased bandwidth.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;homa-api&quot;&gt;Homa API&lt;/h2&gt;

&lt;p&gt;Homa aims to deliver a connectionless transport protocol for RPCs in the data center. The protocol’s approach contrasts with TCP on two dimensions.&lt;/p&gt;

&lt;p&gt;Homa is:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;RPC-oriented, rather than stream-oriented&lt;/em&gt;: TCP’s stream-based approach (which relies on FIFO delivery of messages) can experience high tail latency. The cause of this latency is head of line blocking, where delay in a message at the front (or “head”) of a stream delays the rest of the stream. Homa limits head of line blocking because the protocol does not enforce FIFO ordering of messages.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Connectionless, rather than connection-oriented&lt;/em&gt;: TCP’s connection-oriented approach is not well suited for datacenters because “applications can have hundreds or thousands of them, resulting in high space and time overheads”.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To make the protocol available to developers, the implementation defines an API focused on sending and receiving RPC messages. The primary methods in the API are &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;homa_send&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;homa_recv&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;homa_reply&lt;/code&gt;. These calls operate on sockets that can be reused for many different RPC requests (notably different from TCP). The methods return or accept a 64 bit identifier for a corresponding RPC. Furthermore, an RPC-based approach facilitates abstracting away logic responsible for Homa’s reliability, like the internals of retries.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/homa2/api.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 id=&quot;challenges-in-implementing&quot;&gt;Challenges in implementing&lt;/h2&gt;

&lt;p&gt;The paper outlines three main challenges to&lt;label for=&quot;highspeed&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;highspeed&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;When viewing these challenges, it is important to remember that Homa is designed for reliable high-speed datacenter networks. Thus the constraints Homa faces in the kernel are different than in other, non-datacenter environments. &lt;/span&gt; implementing Homa as a Linux Kernel module:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Moving packets through the protocol stack is costly.&lt;/li&gt;
  &lt;li&gt;Multiple cores are needed to process incoming packets, yet Linux load balancing of this work is non-optimal.&lt;/li&gt;
  &lt;li&gt;Packets need to be assigned priorities and transmitted in real-time. Accomplishing this task with a single core is difficult, using multiple cores to solve the problem even more so.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sending packets is costly, as doing so involves copies and interaction with other Linux features. One approach to this overhead is userspace networking&lt;label for=&quot;zerocopy&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;zerocopy&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Touched on in a past paper review of &lt;a href=&quot;https://www.micahlerner.com/2021/07/07/breakfast-of-champions-towards-zero-copy-serialization-with-nic-scatter-gather.html&quot;&gt;Breakfast of Champions: Towards Zero-Copy Serialization with NIC Scatter-Gather&lt;/a&gt;. &lt;/span&gt;. Another approach mentioned in the paper is batching packets together to amoritize cost - unfortunately, this approach does not work well for Homa because batching packets can introduce latency (a main concern of the protocol).&lt;/p&gt;

&lt;p&gt;Multiple cores are needed to process packets because networks are improving faster than CPUs are&lt;label for=&quot;killermicroseconds&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;killermicroseconds&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;A problem discussed in &lt;a href=&quot;https://cacm.acm.org/magazines/2017/4/215032-attack-of-the-killer-microseconds/fulltext&quot;&gt;this&lt;/a&gt; great ACM article on “Attack of the Killer Microseconds”. &lt;/span&gt;. A challenge to using multiple cores is Linux scheduling, which creates “software congestion” when “too much work is assigned to one core”.&lt;/p&gt;

&lt;p&gt;Lastly, Homa strives to assign priorities to packets, while minimizing the size of the network interface card’s (NIC) transmit queue - more items in this queue means a potentially longer wait time, and more tail latency.&lt;/p&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;As discussed above, there are three primary challenges to implementing Homa as a Linux Kernel module. These challenges impact the sending and receiving path for packets - the visualization below describes these two paths and the components involved in implementing them. Fair warning that the implementation is heavy on Linux internals, and I try to link to documentation for further deep dives where possible!&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/homa2/arch.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 id=&quot;moving-packets&quot;&gt;Moving packets&lt;/h3&gt;

&lt;p&gt;The first challenge in implementing Homa is the cost of moving packets through the networking stack. To solve this problem, the implementation uses batching on the send and receive paths, rather than pushing packets through the stack one by one.&lt;/p&gt;

&lt;p&gt;On the sending path, Homa/Linux uses TCP Segmentation Offload (TSO)&lt;label for=&quot;tso&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;tso&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;There are extensive docs on Linux segmentation offloading &lt;a href=&quot;https://www.kernel.org/doc/Documentation/networking/segmentation-offloads.txt&quot;&gt;here&lt;/a&gt;. Generic Segment Offload is also mentioned, but isn’t supported at this time. &lt;/span&gt;. A TSO-based strategy offloads work to the NIC - the kernel passes large packets to the NIC, which then performs the work of breaking down the packet into smaller segments.&lt;/p&gt;

&lt;p&gt;The implementation of batching on the receive path is somewhat more complicated. When the NIC receives packets, it issues an interrupt. In response to the interrupt, the networking driver schedules a &lt;em&gt;NAPI&lt;/em&gt;&lt;label for=&quot;napi&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;napi&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;NAPI stands for “New API” and is a framework for packet processing. I found this additional documentation on the &lt;a href=&quot;https://wiki.linuxfoundation.org/networking/napi&quot;&gt;Linux Foundation site&lt;/a&gt; useful. &lt;/span&gt; action that polls the NIC for packets until it reaches a configured limit. Once the driver reaches this limit, it communicates batches to the &lt;em&gt;SoftIRQ&lt;/em&gt; layer of the Linux kernel. &lt;em&gt;SoftIRQ&lt;/em&gt; “is meant to handle processing that is almost — but not quite — as important as the handling of hardware interrupts”.&lt;label for=&quot;softirq&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;softirq&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;From Jonathan Corbet’s &lt;a href=&quot;https://lwn.net/Articles/520076/&quot;&gt;2012 article&lt;/a&gt;. Another comprehensive article on receiving data in the Linux Networking Stack is &lt;a href=&quot;https://packagecloud.io/blog/illustrated-guide-monitoring-tuning-linux-networking-stack-receiving-data/&quot;&gt;here&lt;/a&gt;. &lt;/span&gt; Homa builds up messages from the incoming batches, and signals waiting application threads once a message is complete - these applications are then able to make use of the response to the Homa calls mentioned in the API section above.&lt;/p&gt;

&lt;h3 id=&quot;load-balancing&quot;&gt;Load balancing&lt;/h3&gt;

&lt;p&gt;Homa is intended for high speed networks under load. In this environment, a single core is not capable of processing incoming packets - to use multiple cores, Homa must load balance work&lt;label for=&quot;outgoing&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;outgoing&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The paper notes that, “load balancing is easy for packet output, because the output stack executes entirely on the sending thread’s core, with a separate NIC channel per core. The Linux scheduler balances threads across cores, and this distributes the packet transmission load as well.” &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Load balancing is implemented in the kernel with two load balancers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Receive Side Scaling (RSS), which performs load balancing inside the NIC to distribute processing across CPUs. The Linux Networking documentation provides &lt;a href=&quot;https://www.kernel.org/doc/Documentation/networking/scaling.txt&quot;&gt;helpful documentation&lt;/a&gt; on RSS.&lt;/li&gt;
  &lt;li&gt;NAPI (mentioned previously), which performs load balancing at the SoftIRQ layer (once batches of packets are created, those batches need to communicated to waiting application threads)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The paper also mentions that the balancing implementation hurts performance at low load, as “at low load it is best to concentrate all processing on a single core”&lt;label for=&quot;effect&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;effect&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The experimental section of the paper quantifies this effect. &lt;/span&gt;.  While ideally Homa could implement an adaptive load balancing scheme, the paper mentions that “there does not appear to be a way to do this in Linux.” This remark ties into a theme throughout the paper - that the Linux kernel’s focus on TCP (in particular, design impacted by this focus) introduces overhead.&lt;/p&gt;

&lt;h3 id=&quot;real-time-processing&quot;&gt;Real-time processing&lt;/h3&gt;

&lt;p&gt;Homa aims to assign packet priorities and limit the amount of time packets spend in the NIC’s transmit queue - more time in the transmit queue means more delay/potential tail latency. Because the NICs used do not make the size of their transmit queues available, Homa needs to estimate their size. The implementation does so using an estimate of the size of the packets and the link speed. This estimate is updated by a &lt;em&gt;pacer&lt;/em&gt; thread (visible in the architecture diagram above). Unfortunately, there are complications to running the &lt;em&gt;pacer&lt;/em&gt; thread: the pacer can not keep up at high bandwidth, and the operating system scheduler potentially interferes by descheduling the thread’s execution. The paper outlines three workarounds that assist the pacer thread, ensuring it doesn’t fall behind:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Small packets don’t interact with the pacer (meaning less work)&lt;/li&gt;
  &lt;li&gt;Other cores pitch in if the main pacer thread falls behind&lt;/li&gt;
  &lt;li&gt;Other parts of the Homa implementation will queue packets if the thread falls behind&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;

&lt;p&gt;A primary goal of the paper was to evaluate Homa in a production-like environment, reproducing the results of the original Homa paper (covered in &lt;a href=&quot;/2021/08/15/a-linux-kernel-implementation-of-the-homa-transport-protocol.html&quot;&gt;Part I&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;To accomplish this goal, the paper tests the Linux implementation of Homa with four workloads from the original paper. The workloads cover a wide arrange of message sizes (including both small and large RPCs). Furthermore, the paper focuses on cases where there are many clients - Homa is not well suited for situations where there are few RPC clients (arguing that this situation does not arise in data center like environments). The same workloads are executed with TCP and DCTCP (a TCP-like protocol adapted for the datacenter), and compared to Homa’s results.&lt;/p&gt;

&lt;p&gt;The key metric used in this set of performance evaluations is &lt;em&gt;slowdown&lt;/em&gt;. &lt;em&gt;Slowdown&lt;/em&gt; is calculated by comparing the round trip time (RTT) of an RPC to the RTT observed using Homa under ideal conditions (Homa is designed to perform well for small messages on a network under high load). Smaller values of slowdown are better than larger values - larger values for slowdown mean that the result is significantly worse than one would expect from Homa under ideal conditions.&lt;/p&gt;

&lt;p&gt;The graphs below show Homa’s significantly lower slowdown relative to TCP and DCTCP for a variety of message sizes.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/homa2/workloads.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The paper also includes a number of microbenchmarks focused on validating other aspects of the implementation, like how well Homa performs with different numbers of prioritiy levels, or how well the implementation performs under reduced load&lt;label for=&quot;lowload&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;lowload&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Homa is designed for high load, so it is useful to evaluate the implementation in situations it might not otherwise perform well under. &lt;/span&gt;.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The conclusion of the Homa paper asserts that while the implementation “eliminates congestion as a significant performance factor”, remaining software-based overheads pose a future area of improvement. These overheads come from conflicts between Homa and Linux implementation details (like scheduling and load balancing that optimize for TCP).&lt;/p&gt;

&lt;p&gt;The paper discusses two potential solutions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Moving transport protocols to user space&lt;label for=&quot;userspace&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;userspace&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;An interesting topic that will be covered in papers from at least one conference paper over the next few months! &lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;Moving transport protocols to the NIC&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I thoroughly enjoyed diving into Homa - stay tuned for when we resume in the next few weeks. When we will cover papers from OSDI, ATC, and the upcoming set of conferences. Until then!&lt;/p&gt;</content><author><name>Micah</name></author><summary type="html">Programming note: I will be taking a several week break from writing paper reviews for the summer. When we come back, I will be finishing off the papers from Usenix ATC and OSDI, then moving on to the great upcoming conferences (my non-exhaustive list is here). As always, feel free to reach out on Twitter with feedback or suggestions about papers to read! These paper reviews can be delivered weekly to your inbox, or you can subscribe to the new Atom feed.</summary></entry><entry><title type="html">Homa: A Receiver-Driven Low-Latency Transport Protocol Using Network Priorities, Part I</title><link href="http://www.micahlerner.com/2021/08/15/a-linux-kernel-implementation-of-the-homa-transport-protocol.html" rel="alternate" type="text/html" title="Homa: A Receiver-Driven Low-Latency Transport Protocol Using Network Priorities, Part I" /><published>2021-08-15T00:00:00-07:00</published><updated>2021-08-15T00:00:00-07:00</updated><id>http://www.micahlerner.com/2021/08/15/a-linux-kernel-implementation-of-the-homa-transport-protocol</id><content type="html" xml:base="http://www.micahlerner.com/2021/08/15/a-linux-kernel-implementation-of-the-homa-transport-protocol.html">&lt;p&gt;&lt;em&gt;Over the next few weeks I will be reading papers from &lt;a href=&quot;https://www.usenix.org/conference/atc21&quot;&gt;Usenix ATC&lt;/a&gt; and &lt;a href=&quot;https://www.usenix.org/conference/osdi21&quot;&gt;OSDI&lt;/a&gt; - as always, feel free to reach out on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt; with feedback or suggestions about papers to read! These weekly paper reviews can &lt;a href=&quot;https://newsletter.micahlerner.com/&quot;&gt;be delivered weekly to your inbox&lt;/a&gt;, or you can subscribe to the new &lt;a href=&quot;https://www.micahlerner.com/feed.xml&quot;&gt;Atom feed&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://people.csail.mit.edu/alizadeh/papers/homa-sigcomm18.pdf&quot;&gt;Homa: A Receiver-Driven Low-Latency Transport Protocol Using Network Priorities&lt;/a&gt;, Montazeri et al., SIGCOMM 2018&lt;/p&gt;

&lt;p class=&quot;discussion&quot;&gt;Discussion on &lt;a href=&quot;https://news.ycombinator.com/item?id=28204808&quot;&gt; Hacker News&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This week’s paper review is part one of a two-part series on the same research topic - &lt;em&gt;Homa&lt;/em&gt;, a transport protocol purpose-built to replace TCP for low-latency RPC inside the modern data center&lt;label for=&quot;kurose&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;kurose&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;If you are interested in learning more about networking, I can’t recommend the Kurose &amp;amp; Ross book enough. Although it isn’t free, there &lt;em&gt;is&lt;/em&gt; a large amount of course content (like videos), on &lt;a href=&quot;https://gaia.cs.umass.edu/kurose_ross/online_lectures.htm&quot;&gt;their site&lt;/a&gt;. &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Specifically, &lt;em&gt;Homa&lt;/em&gt; aims to replace TCP, which was designed in the era before modern data center environments existed. Consequently, TCP doesn’t take into account the unique properties of data center networks (like high-speed, high-reliability, and low-latency). Furthermore, the nature of RPC traffic is different - RPC communication in a data center often involve enormous amounts of small messages and communication between many different machines. TCP is non-ideal for this type of communication for several reasons - for example, it is designed to ensure reliable transmission of packets (under the assumption that the networks are not reliable), and is a connection-oriented protocol that requires state (meaning that operating many connections at once has a high overhead). The difference between the design goals of TCP and the nature of the data center leads to non-optimal performance under load, which shows up as tail latency&lt;label for=&quot;tail&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;tail&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;For more on tail latency, I’d recommend reading &lt;a href=&quot;https://cacm.acm.org/magazines/2013/2/160173-the-tail-at-scale/fulltext&quot;&gt;The Tail at Scale&lt;/a&gt; - there are also several great reviews of the paper (&lt;a href=&quot;https://blog.acolyer.org/2015/01/15/the-tail-at-scale/&quot;&gt;The Morning Paper&lt;/a&gt;, &lt;a href=&quot;https://squidarth.com/article/systems/2020/02/29/tail-at-scale.html&quot;&gt;Sid Shanker’s blog&lt;/a&gt;, or &lt;a href=&quot;https://www.youtube.com/watch?v=1Qxnrf2pW10&quot;&gt;Vivek Haldar’s video review&lt;/a&gt;). &lt;/span&gt;. To address this issue, researchers and industry created a number of solutions&lt;label for=&quot;newtransport&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;newtransport&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The paper cites &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/1851182.1851192&quot;&gt;DCTCP&lt;/a&gt;, &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/2534169.2486031&quot;&gt;pFabric&lt;/a&gt;, &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3098822.3098825&quot;&gt;NDP&lt;/a&gt;, and &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/2716281.2836086&quot;&gt;pHost&lt;/a&gt;. &lt;/span&gt; purpose built for the data center, of which Homa was the newest.&lt;/p&gt;

&lt;p&gt;I will be publishing this paper review in two parts. The first part gives an overview of the &lt;em&gt;Homa&lt;/em&gt; protocol, based on &lt;em&gt;Homa: A Receiver-Driven Low-Latency Transport Protocol Using Network Priorities&lt;/em&gt; from SIGCOMM 2018. This paper lays out the problems that the research area is trying to solve, designs a solution to those problems, and presents experimental results. The second paper, &lt;em&gt;A Linux Kernel Implementation of the Homa Transport Protocol&lt;/em&gt; was published at this year’s USENIX ATC conference. It discusses the implementation (and challenges to implementing) Homa as a Linux Kernel module, with the goal of evaluating the protocol in a setting that is closer to a real production environment - this paper’s conclusion also discusses the limits of the implementation and a few exciting potential directions for future research.&lt;/p&gt;

&lt;p&gt;With that, let’s dive into understanding Homa.&lt;/p&gt;

&lt;h2 id=&quot;the-homa-protocol&quot;&gt;The Homa protocol&lt;/h2&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;

&lt;p&gt;As discussed above, the problem that &lt;em&gt;Homa&lt;/em&gt; is trying to solve is a disconnect between the design of TCP and the unique qualities of data center networks. This disconnect increases latency and overhead of RPC communications, meaning wasted data center resources. Thus, &lt;em&gt;Homa&lt;/em&gt; is designed with the goal of achieving the “lowest possible latency” for RPC (in particular focusing on small messages at “high network load”).&lt;/p&gt;

&lt;p&gt;To achieve this goal, &lt;em&gt;Homa&lt;/em&gt; must consider a primary source of latency in this type of network: &lt;em&gt;queuing delay&lt;/em&gt;. Queuing delay occurs at routers in a network when more packets arrive than can be transmitted at once&lt;label for=&quot;queue&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;queue&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;For an in depth discussion of delays, I recommend &lt;a href=&quot;https://archive.is/20130114163812/http://59.67.152.66:8000/newenglish/delay.htm&quot;&gt;this&lt;/a&gt; chapter from the Kurose and Ross networking book. &lt;/span&gt; (meaning that they need to wait in a queue). More queuing leads to more latency!&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/homa/delay.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;Queuing delay, sourced from &lt;a href=&quot;http://www.cs.toronto.edu/~marbach/COURSES/CSC358_F19/delay.pdf&quot;&gt;here&lt;/a&gt;.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;To limit queuing, a design could aim to eliminate it entirely or could accept that queueing will happen (while aiming to minimize its negative impact). The paper mentions one system, &lt;a href=&quot;http://fastpass.mit.edu/Fastpass-SIGCOMM14-Perry.pdf&quot;&gt;FastPass&lt;/a&gt;, that implements the first approach using a central scheduler that could theoretically optimally make packet-scheduling decisions. Unfortunately, interacting with the scheduler for every packet “triples the latency” for short messages.&lt;/p&gt;

&lt;p&gt;If queuing is accepted as inherent to the network, the paper argues &lt;em&gt;in-network priorities&lt;/em&gt; must be used to provide finer grained control over how packets are queued&lt;label for=&quot;pfabricq&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;pfabricq&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The paper mentions that previous work demonstrates the positive impact of using these types of priorities. &lt;/span&gt;. &lt;em&gt;In-network priorities&lt;/em&gt; allow a priority to be assigned to a packet, then for that packet to be assigned to a queue that contains only packets with that priority. This ensures that the highest priority packets are transmitted first and provides a degree of control over how different types of traffic is treated.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/homa/priorityq.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;A depiction of a priority queue scheduling system sourced from &lt;a href=&quot;http://www2.ic.uff.br/~michael/kr1999/6-multimedia/6_06-scheduling_and_policing.htm&quot;&gt;this&lt;/a&gt; resource on packet scheduling.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;To determine the priority of a packet, &lt;em&gt;Homa&lt;/em&gt; uses a policy called &lt;em&gt;Shortest Remaining Processing Time first&lt;/em&gt; (SRPT), “which prioritizes packets from messages with the fewest bytes remaining to transmit”. Another way of explaining SRPT is that it aims to schedule packets based on how close the RPC is to completing the transfer of its associated packets. If a packet is associated with an RPC request that has fewer packets left to transmit to a receiver, scheduling that packet first will allow the request to finish faster. The paper mentions that &lt;em&gt;SRPT&lt;/em&gt; is not only common in previous work (like &lt;a href=&quot;https://dl.acm.org/doi/10.1145/2486001.2486031&quot;&gt;pFabric&lt;/a&gt;), but is close to optimal in the conditions that one would see in a network under load.&lt;/p&gt;

&lt;p&gt;Lastly, the paper discusses which parts of the system (client or receiver) should make decisions about the priority of a packet (by appling the SRPT policy) and when. The paper argues that receivers are well positioned to determine packet priorities - they know which clients are sending packets to them and could be configured to keep track of how much data each client has left to send. Even though receivers calculate packet priorities, clients also need to apply SRPT to the packets that they send out (if a client is sending multiple RPCs at once, the RPC that is closest to finishing should have its associated packets sent out first).&lt;/p&gt;

&lt;p&gt;Receivers are also in a position to optimize packet priorities beyond applying the SRPT policy. An example of further optimization is a process called &lt;em&gt;overcommitting&lt;/em&gt;, where the receiver instructs more than one sender to use the same priority at the same time to ensure full network utilization. As mentioned previously, a client might receive information about how to send out packets with optimal priorities, but might delay actually sending out the packets for some reason. One example of this is if a client is sending out multiple RPCs at once and the prioritized packets are delayed client-side while a different RPC is sent out.&lt;/p&gt;

&lt;h3 id=&quot;design-and-implementation&quot;&gt;Design and Implementation&lt;/h3&gt;

&lt;p&gt;Homa is implemented with the concerns above in mind, using receiver-driven priorities decided with the SRPT policy. To accomplish its goals, the system uses four packet types to send data, communicate priorities from receiver to sender, or signal metadata.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/homa/packets.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;When a client wants to send an RPC to a receiver, it sends an initial chunk and metadata that includes the total size of the message (which the receiver will use to track request completion). This chunk is given an &lt;em&gt;unscheduled&lt;/em&gt; priority (as seen in the system diagram below).&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/homa/protocol.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The receiver then applies the SRTP algorithm to decide the priority for the next set of packets associated with the given RPC, then communicates the priority back to the sender using a &lt;em&gt;GRANT&lt;/em&gt; packet. The &lt;em&gt;GRANT&lt;/em&gt; packet instructs the sender to send a configurable number of bytes (called &lt;em&gt;RTT Bytes&lt;/em&gt;) before waiting for another grant. Once the sender receives this information, it sends packets using the &lt;em&gt;scheduled&lt;/em&gt; priority until it reaches the configured limit set via &lt;em&gt;RTT Bytes&lt;/em&gt; (the paper uses 10 KB, but mentions that this number will continue to grow as link speed increases).&lt;/p&gt;

&lt;p&gt;Now that we understand the basics of Homa, it is interesting to contrast the protocol with TCP. Homa forgoes features of TCP (and other RPC systems) that increase overhead and latency:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Explicit acknowledgements&lt;/em&gt;: senders transmit many packets without requiring acknowledgement, occasionally waiting for feedback from the receiver (who provides feedback via &lt;em&gt;GRANT&lt;/em&gt; packets). This approach means fewer packets need to be transmitted as part of the protocol, meaning more bandwidth can be dedicated to transmitting packets that contain RPC data.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Connections&lt;/em&gt;: Homa is connectionless, unlike TCP. Foregoing connections means that Homa does not need to maintain certain types of state, like TCP does. Lower state overhead means Homa is able to service many more RPCs than a TCP-based sender-receiver pair would. Relatedly, the state that Homa maintains is bounded by the RTT bytes configuration parameter - there is a limit to how much data will be transmitted by a sender before waiting for feedback (and a limit to associated data that a single RPC request will consume in the router’s buffers).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;At-most-once delivery semantics&lt;/em&gt;: Other RPC protocols are designed to ensure at-most once-delivery of a complete message, but Homa targets &lt;em&gt;at-least-once&lt;/em&gt; semantics&lt;label for=&quot;atmostonce&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;atmostonce&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;While &lt;a href=&quot;https://www.lightbend.com/blog/how-akka-works-at-least-once-message-delivery&quot;&gt;this guide&lt;/a&gt; focuses on Akka, it is a helpful overview of the different messaging semantics. &lt;/span&gt;. This means that Homa can possibly re-execute RPC requests if there are failures in the network (and an RPC ends up being retried). While at-least-once semantics put a greater burden on the receiving system (which might have to make RPCs idempotent), relaxing the messaging semantics allows Homa receivers to adapt to failures that happen in a data center environment. As an example, Homa receivers can discard state if an RPC becomes inactive, which might happen if a client exceeds a deadline and retries.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The original Homa paper discusses testing the protocol on a variety of workloads - the most recent paper on Homa (covered next week) includes a Linux-compatible implementation and aims to reproduce the evaluation of the protocol in a setting that is closer to one used in production. If you enjoyed this paper review, stay tuned for the next in the series!&lt;/p&gt;</content><author><name>Micah</name></author><summary type="html">Over the next few weeks I will be reading papers from Usenix ATC and OSDI - as always, feel free to reach out on Twitter with feedback or suggestions about papers to read! These weekly paper reviews can be delivered weekly to your inbox, or you can subscribe to the new Atom feed.</summary></entry><entry><title type="html">Systems Conferences 2021</title><link href="http://www.micahlerner.com/2021/08/14/systems-conferences-2021.html" rel="alternate" type="text/html" title="Systems Conferences 2021" /><published>2021-08-14T00:00:00-07:00</published><updated>2021-08-14T00:00:00-07:00</updated><id>http://www.micahlerner.com/2021/08/14/systems-conferences-2021</id><content type="html" xml:base="http://www.micahlerner.com/2021/08/14/systems-conferences-2021.html">&lt;p&gt;In an effort to keep track of conferences that I want to read papers from, I put together an incomplete list. The list is certainly non-exhaustive - feel free to send a pull request with suggestions!&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Conference&lt;/th&gt;
      &lt;th&gt;Date&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;http://cidrdb.org/cidr2021/index.html&quot;&gt;Conference on Innovative Data Systems Research (CIDR)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;January 11-15, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.usenix.org/conference/fast21&quot;&gt;File and Storage Technologies (FAST)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;February 23 – 25, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.usenix.org/conference/nsdi21&quot;&gt;USENIX Symposium on Networked Systems Design and Implementation (NSDI)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;April 12–14, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://2021.eurosys.org/&quot;&gt;EuroSys 2021&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;April 26—28, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://sigops.org/s/conferences/hotos/2021/&quot;&gt;HotOS&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;June 1 - 3, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://2021.sigmod.org/&quot;&gt;SIGMOD/PODS&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;June 20 - 25, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://2021.debs.org/full-program/&quot;&gt;DISTRIBUTED AND EVENT-BASED SYSTEMS (DEBS) 2021&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;June 28 - July 2, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.usenix.org/conference/osdi21&quot;&gt;USENIX Symposium on Operating Systems Design and Implementation (OSDI)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;July 14–16, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.usenix.org/conference/atc21&quot;&gt;USENIX Annual Technical Conference&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;July 14–16, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.hotstorage.org/2021/&quot;&gt;HotStorage&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;July 27–28, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.usenix.org/conference/usenixsecurity21&quot;&gt;USENIX Security&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;August 11–13, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://vldb.org/2021/&quot;&gt;Very Large Data Base (VLDB)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;August 16-20, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://conferences.sigcomm.org/sigcomm/2021/&quot;&gt;SIGCOMM 2021&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;August 23–27, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.usenix.org/conference/srecon21&quot;&gt;SRECon&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;October 12–14, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://sosp2021.mpi-sws.org/&quot;&gt;Symposium on Operating Systems Principles (SOSP)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;October 25-28, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://2021.splashcon.org/&quot;&gt;Systems, Programming, Languages, and Applications: Software for Humanity (SPLASH)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;October 17-22, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://acmsocc.org/2021/&quot;&gt;ACM Symposium on Cloud Computing 2021&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;November 1-3, 2021&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</content><author><name>Micah</name></author><summary type="html">In an effort to keep track of conferences that I want to read papers from, I put together an incomplete list. The list is certainly non-exhaustive - feel free to send a pull request with suggestions!</summary></entry><entry><title type="html">POSH: A Data-Aware Shell</title><link href="http://www.micahlerner.com/2021/08/07/posh-a-data-aware-shell.html" rel="alternate" type="text/html" title="POSH: A Data-Aware Shell" /><published>2021-08-07T00:00:00-07:00</published><updated>2021-08-07T00:00:00-07:00</updated><id>http://www.micahlerner.com/2021/08/07/posh-a-data-aware-shell</id><content type="html" xml:base="http://www.micahlerner.com/2021/08/07/posh-a-data-aware-shell.html">&lt;p&gt;&lt;em&gt;This is the fourth paper in a series on “The Future of the Shell”&lt;label for=&quot;series&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;series&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Here are links to &lt;a href=&quot;/2021/07/14/unix-shell-programming-the-next-50-years.html&quot;&gt;Part 1&lt;/a&gt;, &lt;a href=&quot;/2021/07/24/from-laptop-to-lambda-outsourcing-everyday-jobs-to-thousands-of-transient-functional-containers.html&quot;&gt;Part 2&lt;/a&gt;, and &lt;a href=&quot;/2021/07/31/pash-light-touch-data-parallel-shell-processing.html&quot;&gt;Part 3&lt;/a&gt;. &lt;/span&gt;. These weekly paper reviews can &lt;a href=&quot;https://newsletter.micahlerner.com/&quot;&gt;be delivered weekly to your inbox&lt;/a&gt;, or you can subscribe to the new &lt;a href=&quot;https://www.micahlerner.com/feed.xml&quot;&gt;Atom feed&lt;/a&gt;. Over the next few weeks I will be reading papers from &lt;a href=&quot;https://www.usenix.org/conference/atc21&quot;&gt;Usenix ATC&lt;/a&gt; and &lt;a href=&quot;https://www.usenix.org/conference/osdi21&quot;&gt;OSDI&lt;/a&gt; - as always, feel free to reach out on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt; with feedback or suggestions about papers to read!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.usenix.org/conference/atc20/presentation/raghavan&quot;&gt;POSH: A Data-Aware Shell&lt;/a&gt; Deepti Raghavan, et. al&lt;/p&gt;

&lt;p class=&quot;discussion&quot;&gt;Discussion on &lt;a href=&quot;https://news.ycombinator.com/item?id=28108347&quot;&gt; Hacker News&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This week’s paper review covers &lt;em&gt;POSH&lt;/em&gt;, a system capable of achieving dramatic speedups for &lt;em&gt;unmodified&lt;/em&gt;&lt;label for=&quot;adoption&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;adoption&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Requiring fewer changes to a shell script in order to make it POSH-compatible simplifies adoption. &lt;/span&gt; shell scripts that perform large amounts of IO - intriguing use cases of POSH are log analysis or the git workflows of large software projects&lt;label for=&quot;git&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;git&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The paper analyzes git workflows for &lt;a href=&quot;https://github.com/chromium/chromium&quot;&gt;Chromium&lt;/a&gt;. &lt;/span&gt;. In particular, POSH shines in environments that use distributed file systems like Network File System&lt;label for=&quot;nfs&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;nfs&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;NFS allows you to “mount” a remote disk to your computer and then sends RPC calls to a remote server to perform file accesses. I highly recommend &lt;a href=&quot;https://pages.cs.wisc.edu/~remzi/OSTEP/dist-nfs.pdf&quot;&gt;this&lt;/a&gt; amazing (and free!) description of NFS from &lt;em&gt;Operating Systems: Three Easy Pieces&lt;/em&gt;. The entire book is available for free online &lt;a href=&quot;https://pages.cs.wisc.edu/~remzi/OSTEP/&quot;&gt;here&lt;/a&gt;.  &lt;/span&gt; (NFS) mounts - I’ve included a link to a great overview of NFS in the sidebar (or if you are on mobile, you can click the number “4” to reveal it).&lt;/p&gt;

&lt;p&gt;POSH achieves speedups by minimizing data transfers in scripts that use networked storage. To minimize data transfers, POSH can execute parts of a script that read or write remote files in a process on the remote machine. As an example, consider a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep&lt;/code&gt; of a file stored on a remote machine. When a client&lt;label for=&quot;client&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;client&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Client in this case meaning the computer where the script was initiated by a user. &lt;/span&gt; computer attempts to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep&lt;/code&gt; the file, the shell will transfer &lt;em&gt;the whole file&lt;/em&gt; over the network to the client node, then filter the file on the client. In contrast, POSH can perform the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep&lt;/code&gt; on the remote storage server, and only transfer the filtered output back to the client, dramatically lowering network traffic.&lt;/p&gt;

&lt;p&gt;To make decisions about which parts of a script are executed remotely, POSH produces a graph representation of the shell script’s execution - the nodes in the graph are commands, while the edges represent the flow of data between commands. Correctly transforming a shell script into this graph representation is a nuanced, yet critical function. To facilitate it, POSH leverages an annotation language capable of describing a given command’s parameters, inputs, and outputs (as well as a number of important configuration options).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;POSH&lt;/em&gt; and the system described in &lt;a href=&quot;/2021/07/31/pash-light-touch-data-parallel-shell-processing.html&quot;&gt;last week’s paper review&lt;/a&gt;, &lt;em&gt;PaSh&lt;/em&gt;, are similar in that they both aim to speedup shell script execution without requiring modifications to the original script. Additionally, they both leverage annotations of shell commands in their implementations. Even though the two projects are similar in some respects, PaSh and POSH focuses on different uses cases - PaSH focuses on parallelizing “trivially parallelizable” computation local to a machine, while POSH focuses on parallelizing scripts that perform large amounts of IO across remote machines. Both projects are part of an exciting (and high impact) thread of research related to modernizing the shell, and I’m looking forward to seeing more from the two teams!&lt;/p&gt;

&lt;h2 id=&quot;what-are-the-papers-contributions&quot;&gt;What are the paper’s contributions?&lt;/h2&gt;

&lt;p&gt;The paper makes two contributions. The first is an &lt;em&gt;annotation language&lt;/em&gt; that describes a shell command. These command specifications are used to transform the script into a graph representation - the different steps of a script’s execution are the nodes, and the data flow between those nodes are the edges. The second contribution is a &lt;em&gt;scheduling algorithm&lt;/em&gt; that decides how the steps in a script should be executed, taking into account the dependencies in the script’s graph representation as well as the interactions that a step has with remote storage.&lt;/p&gt;

&lt;p&gt;Before we dive into the details of these two contributions, it is first helpful to understand POSH’s three high level components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Annotation interface&lt;/em&gt;: As mentioned above, the annotation language allows a shell script to be correctly transformed into a graph representation.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Parser and scheduler&lt;/em&gt;: The parser uses the aforementioned annotations to produce a graph representation of a shell script. The scheduler uses this graph representation to assign the execution of steps to remote or local nodes called &lt;em&gt;proxy servers&lt;/em&gt;. The internals of the scheduling process are detailed later on in this paper review.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Execution engine&lt;/em&gt;: Once the scheduler has assigned work to a &lt;em&gt;proxy server&lt;/em&gt;, that work will be executed, and the result will be transferred over the network back to the client node.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/posh/system.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;High-level POSH overview&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 id=&quot;shell-annotation-language&quot;&gt;Shell annotation language&lt;/h2&gt;

&lt;p&gt;POSH uses its shell annotation language to describe the constraints of any given shell command’s execution. These annotations are then used to transform a shell script into a correct graph representation that, when scheduled, will accomplish POSH’s goal of minimizing network traffic.&lt;/p&gt;

&lt;p&gt;The paper outlines three questions that POSH (and the annotation language) must answer to achieve the system’s goals:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Which commands can be executed on remote nodes (called proxy servers)?&lt;/em&gt;: this is important for determining what must run locally, versus what can run on a remote &lt;em&gt;proxy server&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Do any commands in a provided script “filter their input”?&lt;/em&gt;: knowing if a command does or does not filter its input is useful for determining whether it should be executed remotely in conjunction with other commands. The paper provides the example of a executing a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat&lt;/code&gt; followed by a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep&lt;/code&gt; on the same remote &lt;em&gt;proxy server&lt;/em&gt; - as “cat usually produces the same amount of output as input, but grep usually filters its input, POSH must also offload grep” to minimize network traffic.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Can a command be parallelized?&lt;/em&gt;: to enable optimal scheduling, POSH should aim to parallelize a command as much as possible. Without an annotation language, the system might not be have the information it needs to make scheduling decisions. One motivating example is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat file1 file2 file3&lt;/code&gt; - the annotation language defines that the inputs to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat&lt;/code&gt; are “splittable”, meaning that it might be possible run the three commands &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat file&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat file2&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat file3&lt;/code&gt; in parallel on different machines.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I wanted to note two important components of the annotation language important to understanding the rest of the paper&lt;label for=&quot;opt&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;opt&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The paper provides a significant amount of detail on the annotation language and I highly recommend referring to the original paper if this is interesting to you! &lt;/span&gt;. First, the annotations can be defined per command and per argument - this flexibility is important because different arguments to a command can change its behavior and arguments. Second, a command’s inputs/outputs can be typed, and its behavior is defined. For example, the annotation language can indicate a command’s parallelizablity&lt;label for=&quot;p&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;p&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;As an example, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat&lt;/code&gt; is annotated with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;splittable&lt;/code&gt; to indicate that it is potentially parallelizable. &lt;/span&gt; or whether the command relies on the current directory&lt;label for=&quot;git&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;git&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;An example being &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git add&lt;/code&gt; is dependent on the current directory. &lt;/span&gt;. Defining these properties of a command allow the parser and scheduler to answer the three questions above.&lt;/p&gt;

&lt;p&gt;The next section covers how a graph representation of a shell script, produced by passing the shell script through POSH’s parser, is scheduled and executed.&lt;/p&gt;

&lt;h2 id=&quot;scheduling&quot;&gt;Scheduling&lt;/h2&gt;

&lt;p&gt;As discussed above, each shell script is passed through the POSH parser to produce a graph representation. The nodes in the graph representation are then scheduled to execute based on a two step process that &lt;em&gt;resolves scheduling constraints&lt;/em&gt; and &lt;em&gt;minimizes network transfers&lt;/em&gt;.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/posh/dag.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The first step of scheduling, &lt;em&gt;resolving constraints&lt;/em&gt;, determines whether any nodes in the graph &lt;em&gt;must&lt;/em&gt; run on a given remote machine (and if so, which one). Scheduling constraints are created for a variety of reasons - one example constraint is for a command that accesses remote files. To avoid transferring the whole file over the network, that command &lt;em&gt;must&lt;/em&gt; be scheduled on the remote node.&lt;/p&gt;

&lt;p&gt;The second step, &lt;em&gt;minimizing data transfer&lt;/em&gt;, assigns commands to a remote machine if the command was not assigned in the first step. For this assignment, POSH makes use of some graph theory and implements an algorithm using &lt;em&gt;sources&lt;/em&gt;, &lt;em&gt;sinks&lt;/em&gt;, and &lt;em&gt;paths&lt;/em&gt;&lt;label for=&quot;yegge&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;yegge&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;To quote Steve Yegge, “Graphs are, like, really really important.” &lt;/span&gt;. A &lt;em&gt;source&lt;/em&gt; is a “file that is read”, a &lt;em&gt;sink&lt;/em&gt; is the “output file that is written to”, and &lt;em&gt;paths&lt;/em&gt; connect them. To assign nodes, POSH iterates over every source node, checking whether the sink and source node in the path are already assigned to the same machine - if they are, assign all the intermediate nodes along the path to that machine as well! If the sink is not on the same machine, “the scheduler must find the edge along which cross-location data transfer should occur: to minimize data transfer, this should be the edge where the least data flows.” The paper describes a set of heuristics (&lt;a href=&quot;https://github.com/deeptir18/posh/blob/151b0729c4c45829485619c497506a264b0fea02/shell/src/scheduler/heuristic.rs#L37&quot;&gt;implemented here in Rust!&lt;/a&gt;) used to find the min-cut edge in the path. After this edge is found, unassigned nodes are scheduled to run on the machine that the source or sink is scheduled for, “depending on if the node is before or after the minimum cut edge”.&lt;/p&gt;

&lt;h2 id=&quot;applying-and-evaluating-posh&quot;&gt;Applying and evaluating POSH&lt;/h2&gt;

&lt;p&gt;POSH was evaluted on the time it takes to execute a number of applications. This paper review focuses on two specific applications: a distributed log analyis and a git workflow for Chromium. The experimental configuration involved using either a cloud-to-cloud setup (where client and machines are in the cloud) or a university-to-cloud setup (where the POSH client is located at Stanford). The cloud-to-cloud setup has significantly higher bandwidth and significantly lower RTT, and helps to demonstrate that POSH is capable of achieving speedups even with a more powerful network.&lt;/p&gt;

&lt;p&gt;The baseline performance measurement in these experiments comes from exercising each application using NFS instead of POSH. The NFS-only setup mimics a situation where the applications would perform IO-heavy workloads, but be unable to parallelize them (nor be able to limit network overhead).&lt;/p&gt;

&lt;p&gt;For the distributed log analysis (which involves searching for an IP address in a 15GB log dump), POSH sees a speedup from parallelizing across multiple NFS mounts in both experimental setups, although POSH sees a more dramatic speedup in the university-to-cloud setup than in the cloud-to-cloud setup (12.7x improvement in the former versus 2x improvement in the latter).&lt;/p&gt;

&lt;p&gt;For the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git&lt;/code&gt; workflow experiment, git operations (like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git status&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git add&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git commit&lt;/code&gt;) were exercised by reverting, then recommitting 20 commits from the (quite large) Chromium open source project - &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git&lt;/code&gt; commands on such a large project make many metadata calls (to determine whether a file has changed, for example). POSH shines in this experiment, achieving a 10-15x latency improvement in the cloud-to-cloud environment. This application seems incredibly useful - in the past, I’ve read about Facebook’s efforts to &lt;a href=&quot;https://engineering.fb.com/2014/01/07/core-data/scaling-mercurial-at-facebook/&quot;&gt;scale Mercurial&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;POSH is a novel system for parallelizing IO-intensive shell scripts by performing work “close to the data”. The paper is one component of an exciting thread of research that could lead to significant improvements to user experience - given that technical folks from many different backgrounds use the shell every day, these improvements would be high impact.&lt;/p&gt;

&lt;p&gt;Next week I will move on from this series and into papers from Usenix ATC and OSDI. As always, feel free to reach out on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt; with feedback or suggestions about papers to read. Until next time!&lt;/p&gt;</content><author><name>Micah</name></author><summary type="html">This is the fourth paper in a series on “The Future of the Shell”Here are links to Part 1, Part 2, and Part 3. . These weekly paper reviews can be delivered weekly to your inbox, or you can subscribe to the new Atom feed. Over the next few weeks I will be reading papers from Usenix ATC and OSDI - as always, feel free to reach out on Twitter with feedback or suggestions about papers to read!</summary></entry><entry><title type="html">PaSh: Light-touch Data-Parallel Shell Processing</title><link href="http://www.micahlerner.com/2021/07/31/pash-light-touch-data-parallel-shell-processing.html" rel="alternate" type="text/html" title="PaSh: Light-touch Data-Parallel Shell Processing" /><published>2021-07-31T00:00:00-07:00</published><updated>2021-07-31T00:00:00-07:00</updated><id>http://www.micahlerner.com/2021/07/31/pash-light-touch-data-parallel-shell-processing</id><content type="html" xml:base="http://www.micahlerner.com/2021/07/31/pash-light-touch-data-parallel-shell-processing.html">&lt;p&gt;&lt;em&gt;This week’s paper review is the third in a series on “The Future of the Shell”&lt;label for=&quot;series&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;series&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Here are links to &lt;a href=&quot;/2021/07/14/unix-shell-programming-the-next-50-years.html&quot;&gt;Part 1&lt;/a&gt; and &lt;a href=&quot;/2021/07/24/from-laptop-to-lambda-outsourcing-everyday-jobs-to-thousands-of-transient-functional-containers.html&quot;&gt;Part 2&lt;/a&gt; &lt;/span&gt;. These weekly paper reviews can &lt;a href=&quot;https://newsletter.micahlerner.com/&quot;&gt;be delivered weekly to your inbox&lt;/a&gt;, and based on feedback last week I added an &lt;a href=&quot;https://www.micahlerner.com/feed.xml&quot;&gt;Atom feed&lt;/a&gt; to the site. Over the next few weeks I will be reading papers from &lt;a href=&quot;https://www.usenix.org/conference/atc21&quot;&gt;Usenix ATC&lt;/a&gt; and &lt;a href=&quot;https://www.usenix.org/conference/osdi21&quot;&gt;OSDI&lt;/a&gt; - as always, feel free to reach out on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt; with feedback or suggestions about papers to read!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://doi.org/10.1145/3447786.3456228&quot;&gt;PaSh: Light-touch Data-Parallel Shell Processing&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This week’s paper discusses &lt;em&gt;PaSh&lt;/em&gt;, a system designed to automatically parallelize shell scripts. It accomplishes this goal by transforming a script into a graph of computation, then reworking the graph to enhance parallelism. To ensure that this transformation process is correct, PaSh analyzes a command as annotated with a unique configuration language - this configuration explicitly defines a command’s inputs, outputs, and parallelizability. The final step in applying PaSh transforms the intermediate graph of computation back into a shell script, although the rewritten version of the shell script will (hopefully) be able to take advantage of greater parallelism.&lt;/p&gt;

&lt;p&gt;On its own, PaSh can provide dramatic speedups to shell scripts (applying the system to common Unix one-liners accelerates them by up to 60x!), and it would certainly be interesting to see it coupled with other recent innovations in the shell (as discussed in other paper reviews in this series).&lt;/p&gt;

&lt;h2 id=&quot;what-are-the-papers-contributions&quot;&gt;What are the paper’s contributions?&lt;/h2&gt;

&lt;p&gt;The paper makes three main contributions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A study of shell commands and their parallelizability. The outcome of this study are categorizations of shell commands based on their behavior when attempting to parallelize them, and an annotation language used to describe this behavior.&lt;/li&gt;
  &lt;li&gt;A dataflow model&lt;label for=&quot;dataflow&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;dataflow&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;There are many interesting papers that implement a dataflow model, although a foundational one is &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2007/03/eurosys07.pdf&quot;&gt;Dryad: Distributed Data-Parallel Programs from Sequential Building Blocks&lt;/a&gt; &lt;/span&gt; useful for representing a shell script. A given script’s model is informed by the commands in it and the behavior of those commands (each command’s behavior is described with the custom annotation language mentioned above). The intermediate dataflow model is reworked to enhance parallelism, and is finally transformed back into a shell script (which then executes a more parallel version of the original computation).&lt;/li&gt;
  &lt;li&gt;A runtime capable of executing the output shell script, including potentially custom steps that the PaSh system introduces into the script to facilitate parallelization.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;shell-commands-and-their-parallelizability&quot;&gt;Shell commands and their parallelizability&lt;/h2&gt;

&lt;p&gt;PaSh seeks to parallelize shell commands, so the author’s first focus on enumerating the behaviors of shell commands through this lens. The result is four categories of commands:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Stateless&lt;/em&gt; commands, as the name suggests, don’t maintain state and are analagous to a “map” function. An example of stateless commands are &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tr&lt;/code&gt;, or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;basename&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Parallelizable pure&lt;/em&gt; commands produce the “same outputs for same inputs — but maintain internal state across their entire pass”. An example is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sort&lt;/code&gt; (which should produce the same sorted output for the same input, but needs to keep track of all elements) or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wc&lt;/code&gt; (which should produce the same count for a given input, but needs to maintain a counter).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Non-parallelizable pure&lt;/em&gt; commands are purely functional (as above, same outputs for same inputs), but “cannot be parallelized within a single data stream”. An example command is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sha1sum&lt;/code&gt;, which hashes input data - if the stream of data passed to the command is evaluated in a different order, a different output will be generated. This is contrast to a &lt;em&gt;parallelizable pure&lt;/em&gt; command like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sort&lt;/code&gt; - no matter which order you pass in unsorted data, the same output will be produced, making the computation it performs parallelizable.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Side-effectful&lt;/em&gt; commands alter the state of the system, “for example, updating environment variables, interacting with the filesystem, and accessing the network.”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This categorization system is applied to commands in POSIX and GNU Coreutils.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/pash/parclasses.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;While the vast majority of commands (in the &lt;em&gt;non-parallizable pure&lt;/em&gt; and &lt;em&gt;side-effectful&lt;/em&gt; categories) can not be parallelized without significant complication, a significant portion of commands can be relatively-easily parallelized (&lt;em&gt;stateless&lt;/em&gt; or &lt;em&gt;parallelizable pure&lt;/em&gt; categories).&lt;/p&gt;

&lt;h2 id=&quot;shell-command-annotation&quot;&gt;Shell command annotation&lt;/h2&gt;

&lt;p&gt;To describe the &lt;em&gt;parallelizability class&lt;/em&gt; of a given command and argument, the paper’s authors built an &lt;em&gt;extensibility framework&lt;/em&gt;. This framework contains two components: an &lt;em&gt;annotation language&lt;/em&gt; and &lt;em&gt;custom aggregators&lt;/em&gt; which collect the output of commands executing in parallel and stream output to the next command in the script.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;annotation language&lt;/em&gt;&lt;label for=&quot;textproto&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;textproto&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The annotation language is defined in JSON which seems helpful for initial development - it would be interesting to see the language ported to a more constrained format, like protobuf/textproto. &lt;/span&gt; is used to produce &lt;em&gt;annotations&lt;/em&gt; that indicate inputs, outputs, and parallelization class on a per-command basis&lt;label for=&quot;opensource&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;opensource&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The entirety of PaSh, including annotations, are &lt;a href=&quot;https://github.com/binpash/pash&quot;&gt;open source on GitHub&lt;/a&gt; - the docs include a &lt;a href=&quot;https://github.com/binpash/pash/blob/main/annotations/README.md#how-to-annotate-a-command&quot;&gt;useful reference for annotating commands&lt;/a&gt;. &lt;/span&gt;. An example for the default behavior of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cut&lt;/code&gt; command is below:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{ &quot;command&quot;: &quot;cut&quot;, { &quot;predicate&quot;: &quot;default&quot;, &quot;class&quot;: &quot;stateless&quot;, &quot;inputs&quot;: [&quot;stdin&quot;], &quot;outputs&quot;: [&quot;stdout&quot;] }&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;One can also define behavior for specific command and argument combinations. For example, providing (or omitting) a specific argument might change the parallelizability class or inputs/outputs for a command - an example of this is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cut&lt;/code&gt; with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-z&lt;/code&gt; operand (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cut&lt;/code&gt; reads from stdin if the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-z&lt;/code&gt; operand is not provided):&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{ &quot;predicate&quot;: {&quot;operator&quot;: &quot;exists&quot;, &quot;operands&quot;: [ &quot;-z&quot; ]}, &quot;class&quot;: &quot;n-pure&quot;, &quot;inputs&quot;: [ &quot;args[:]&quot; ], &quot;outputs&quot;: [ &quot;stdout&quot; ] }&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The second component of the extensibility framework are &lt;em&gt;custom aggregators&lt;/em&gt; that coalesce the results of parallelized operations into a result stream - an example aggregator takes in two streams of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wc&lt;/code&gt; results and produces an element in an output stream. The &lt;a href=&quot;https://github.com/binpash/pash/tree/main/runtime/agg/py&quot;&gt;PaSh open source project&lt;/a&gt; includes a more complete set of examples that can aggregate the results of other parallelized commands, like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uniq&lt;/code&gt;.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/pash/agg.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;Example aggregator that takes two input streams and reduces them to an output&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The implementation of PaSh relies on the annotation language and custom aggregators described above in order to transform a shell script into an intermediate state where the script’s commands are represented with a &lt;em&gt;dataflow model&lt;/em&gt; - this transformation is covered in the next section.&lt;/p&gt;

&lt;h2 id=&quot;transforming-a-script&quot;&gt;Transforming a script&lt;/h2&gt;

&lt;p&gt;There are multiple steps involved in transforming a script to a more-parallel form. First, PaSh parses it, then transforms the parsed script into an intermediate state called a &lt;em&gt;dataflow graph&lt;/em&gt; (DFG) model. The annotation language facilitates this transformation by providing hints to the system about each step’s inputs, outputs, and behavior when parallelized. The intermediate dataflow model can be reconfigured to produce a more parallel of the script’s associated commands. Lastly, the resulting graph is transformed back into a shell script.&lt;/p&gt;

&lt;p&gt;PaSh implements this transformation process end-to-end using three components: a &lt;em&gt;frontend&lt;/em&gt;, the &lt;em&gt;dataflow model&lt;/em&gt;, and the &lt;em&gt;backend&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;frontend&lt;/em&gt; first parses the provided script into an Abstract Syntax Tree (AST). From there, it “performs a depth-first search” on the AST of the shell script, building &lt;em&gt;dataflow regions&lt;/em&gt; as it goes along - a &lt;em&gt;dataflow region&lt;/em&gt; corresponds to a section of the script that can be parallelized without hitting a “barrier” that enforces sequential execution&lt;label for=&quot;careful&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;careful&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;It’s worth noting that PaSh also is very conservative when it comes to building &lt;em&gt;dataflow regions&lt;/em&gt; that it might try to parallelize later - if an annotation for a command is not defined or it is possible that the parallelizability class of an expression could be altered (for example, if the command relies on an environment variable), then PaSh will not parallelize it. &lt;/span&gt;. Examples of barriers are &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;amp;&amp;amp;&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;||&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;;&lt;/code&gt;.  The resulting &lt;em&gt;dataflow model&lt;/em&gt; is essentially a graph where the nodes are commands and edges indicate command inputs or outputs.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/pash/dfg.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Once the dataflow graphs (DFG) for a script are produced, they can be reworked to take advantage of user-configured parallelism called &lt;em&gt;width&lt;/em&gt;. The paper lays out the formal basis for this transformation - as an example, stateless and parallelizable pure commands can be reconfigured to enhance parallelism while still producing the same result (the paper presents the idea that for these two parallelizability classes, the result of reordering nodes is the same).&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/pash/stateless.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The paper also mentions other useful transformations of the graph - one in particular, adding a relay node between two nodes, is useful for enhancing performance (as described in the next section).&lt;/p&gt;

&lt;p&gt;After all transformations are performed on the graph, the &lt;em&gt;Backend&lt;/em&gt; transforms the graph back into an executable shell script.&lt;/p&gt;

&lt;h2 id=&quot;runtime&quot;&gt;Runtime&lt;/h2&gt;

&lt;p&gt;The output script generated by the application of PaSh can be difficult to execute for several reasons outlined by the paper, although this paper review doesn’t include a complete description of the runtime challenges&lt;label for=&quot;runtime&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;runtime&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The paper does a better job of noting them than I could and I highly recommend digging in! &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;One particularly important detail of the runtime is an approach to overcoming the shell’s “unusual laziness” - the paper notes that the “shell’s evaluation strategy is unusually lazy, in that most commands and shell constructs consume their inputs only when they are ready to process more.” To ensure high resource utilization, PaSh inserts “relay nodes”, which “consume input eagerly while attempting to push data to the output stream, forcing upstream nodes to produce output when possible while also preserving task-based parallelism”. For a number of reasons, other approaches to solving the “eagerness” problem (like not addressing it or using files) result in less performant or even possibly incorrect implementations - this comes into play in the evaluation section of the paper.&lt;/p&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;

&lt;p&gt;The evaluation section of the PaSh paper includes a number of applications, but I choose to focus on three applications that stuck out to me: Common Unix One-liners, NOAA weather analysis, and Wikipedia web indexing.&lt;/p&gt;

&lt;p&gt;Applying PaSh to the set of common UNIX one-liners exercises a variety of different scripts that use &lt;em&gt;stateless&lt;/em&gt;, &lt;em&gt;parallizable pure&lt;/em&gt;, and &lt;em&gt;non-parallelizable pure&lt;/em&gt; in different configurations and numbers, speeding up scripts by up to 60x. This set of tests also demonstrates that implementation details like eager evaluating (outlined in the &lt;em&gt;Runtime&lt;/em&gt; section above) make a difference&lt;label for=&quot;benchmark&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;benchmark&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;This result is shown by benchmarking against versions of PaSh without the implementation or with a different, blocking implementation. &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;The next example I chose applies PaSh to an example data pipeline that analyzes NOAA weather data. PaSh is applied to the entire pipeline and achieves significant speedups - this example is particularly useful at demonstrating that the system can help to parallelize non-compute bound pipelines (the NOAA example downloads a signficant amount of data over the network). In particular, downloading large amounts of data over the network seems to closely relate to ideas discussed in the &lt;a href=&quot;/2021/07/14/unix-shell-programming-the-next-50-years.html&quot;&gt;first paper in this series&lt;/a&gt;, which mentions avoding redundant computation - parallelizing network requests automatically while ensuring that none are repeated unnecessarily (if the script was rerun or slightly changed) would be amazing!&lt;/p&gt;

&lt;p&gt;The last example I choose to include in this evaluation section is of Wikipedia web indexing - PaSh is able to achieve a 12X speedup when extracting text from a large body of Wikipedia’s HTML. This example uses scripts written in Python and Javascript, showcasing PaSh’s ability to speedup a pipeline utilizing commands from many different langauges and why the shell is still such a useful tool.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;PaSh presents an intriguing system for automatically transforming shell scripts into more-parallel versions of themselves. I was particularly interested in how PaSh accomplishes its goals by leveraging annotations of shell commands and arguments - it would be interesting to see an open source community sprout up around maintaining or generating these annotations&lt;label for=&quot;homebrew&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;homebrew&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;I felt some similarity to Mac’s &lt;a href=&quot;https://brew.sh/&quot;&gt;Homebrew&lt;/a&gt;, where users define recipes for downloading different open source projects. &lt;/span&gt;. PaSh’s use of a dataflow-based architecture also demonstrates how powerful the paradigm is. Last but not least, I’m looking forward to seeing how a system like PaSh could fit in with other related innovations in the shell (like next week’s paper on POSH)!&lt;/p&gt;

&lt;p&gt;As always, if you have feedback feel free to reach on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt;. Until next time!&lt;/p&gt;</content><author><name>Micah</name></author><summary type="html">This week’s paper review is the third in a series on “The Future of the Shell”Here are links to Part 1 and Part 2 . These weekly paper reviews can be delivered weekly to your inbox, and based on feedback last week I added an Atom feed to the site. Over the next few weeks I will be reading papers from Usenix ATC and OSDI - as always, feel free to reach out on Twitter with feedback or suggestions about papers to read!</summary></entry><entry><title type="html">From Laptop to Lambda: Outsourcing Everyday Jobs to Thousands of Transient Functional Containers</title><link href="http://www.micahlerner.com/2021/07/24/from-laptop-to-lambda-outsourcing-everyday-jobs-to-thousands-of-transient-functional-containers.html" rel="alternate" type="text/html" title="From Laptop to Lambda: Outsourcing Everyday Jobs to Thousands of Transient Functional Containers" /><published>2021-07-24T00:00:00-07:00</published><updated>2021-07-24T00:00:00-07:00</updated><id>http://www.micahlerner.com/2021/07/24/from-laptop-to-lambda-outsourcing-everyday-jobs-to-thousands-of-transient-functional-containers</id><content type="html" xml:base="http://www.micahlerner.com/2021/07/24/from-laptop-to-lambda-outsourcing-everyday-jobs-to-thousands-of-transient-functional-containers.html">&lt;p&gt;&lt;em&gt;This week’s paper review is the second in a series on “The Future of the Shell” (Part 1, a paper about possible ways to innovate in the shell is &lt;a href=&quot;/2021/07/14/unix-shell-programming-the-next-50-years.html&quot;&gt;here&lt;/a&gt;). As always, feel free to reach out on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt; with feedback or suggestions about papers to read! These weekly paper reviews can also &lt;a href=&quot;https://newsletter.micahlerner.com/&quot;&gt;be delivered weekly to your inbox&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.usenix.org/system/files/atc19-fouladi.pdf&quot;&gt;From Laptop to Lambda: Outsourcing Everyday Jobs to Thousands of Transient Functional Containers&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This week’s paper discusses &lt;em&gt;gg&lt;/em&gt;, a system designed to  parallelize commands initiated from a developer desktop using cloud functions&lt;label for=&quot;firecracker&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;firecracker&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Like those running on AWS Lambda in &lt;a href=&quot;/2021/06/17/firecracker-lightweight-virtualization-for-serverless-applications.html&quot;&gt;Firecracker VMs, as discussed in a previous paper review&lt;/a&gt;. &lt;/span&gt; - an alternative summary is that &lt;em&gt;gg&lt;/em&gt; allows a developer to, for a limited time period, “rent a supercomputer in the cloud”.&lt;/p&gt;

&lt;p&gt;While parallelizing computation using cloud functions is not a new idea on its own&lt;label for=&quot;simdiff&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;simdiff&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Related systems, like &lt;a href=&quot;/2021/06/27/ray-a-distributed-framework-for-emerging-ai-applications.html&quot;&gt;Ray&lt;/a&gt;, are discussed later in this paper review. &lt;/span&gt;, &lt;em&gt;gg&lt;/em&gt; focuses specifically on leveraging affordable cloud compute functions to speed up applications not natively designed for the cloud, like &lt;a href=&quot;https://www.gnu.org/software/make/&quot;&gt;make&lt;/a&gt;-based build systems (common in open source projects), unit tests, and video processing pipelines.&lt;/p&gt;

&lt;h2 id=&quot;what-are-the-papers-contributions&quot;&gt;What are the paper’s contributions?&lt;/h2&gt;

&lt;p&gt;The paper’s contains two primary contributions: the design and implementation of &lt;em&gt;gg&lt;/em&gt; (a general system for parallelizing command line operations using a computation graph executed with cloud functions) and the application of &lt;em&gt;gg&lt;/em&gt; to several domains (including unit testing, software compilation, and object recognition).&lt;/p&gt;

&lt;p&gt;To accomplish the goals of &lt;em&gt;gg&lt;/em&gt;, the authors needed to overcome three challenges: managing software dependencies for the applications running in the cloud, limiting round trips from the developer’s workstation to the cloud (which can be incurred if the developer’s workstation coordinates cloud executions), and making use of cloud functions themselves.&lt;/p&gt;

&lt;p&gt;To understand the paper’s solutions to these problems, it is helpful to have context on several areas of related work:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Process migration and outsourcing&lt;/em&gt;: &lt;em&gt;gg&lt;/em&gt; aims to outsource computation from the developer’s workstation to remote nodes. Existing systems like &lt;a href=&quot;https://distcc.github.io/&quot;&gt;distcc&lt;/a&gt; and &lt;a href=&quot;https://github.com/icecc/icecream&quot;&gt;icecc&lt;/a&gt; use remote resources to speed up builds, but often require long-lived compute resources, potentially making them more expensive to use. In contrast, &lt;em&gt;gg&lt;/em&gt; uses cloud computing functions that can be paid for at the second or millisecond level.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Container orchestration systems&lt;/em&gt;: &lt;em&gt;gg&lt;/em&gt; runs computation in cloud functions (effectively containers in the cloud). Existing container systems, like Kubernetes or Docker Swarm, focus on the actual scheduling and execution of tasks, but don’t necessarily concern themselves with executing dynamic computation graphs - for example, if Task B’s inputs are the output of Task A, how can we make the execution of Task A fault tolerant and/or memoized.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Workflow systems&lt;/em&gt;: &lt;em&gt;gg&lt;/em&gt; transforms an application into small steps that can be executed in parallel. Existing systems following a similar model (like Spark) need to be be programmed for specific tasks, and are not designed for “everyday” applications that a user would spawn from the command line. While Spark can call system binaries, the binary is generally installed on all nodes, where each node is long-lived. In contast, &lt;em&gt;gg&lt;/em&gt; strives to provide the minimal dependencies and data required by a specific step - the goal of limiting dependencies also translates into lower overhead for computation, as less data needs to be transferred before a step can execute. Lastly, systems like Spark are accessed through language bindings, whereas &lt;em&gt;gg&lt;/em&gt; aims to be language agnostic.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Burst-parallel cloud functions&lt;/em&gt;: &lt;em&gt;gg&lt;/em&gt; aims to be a higher-level and more general system for running short-lived cloud functions than existing approaches - the paper cites &lt;a href=&quot;http://pywren.io/&quot;&gt;PyWren&lt;/a&gt; and &lt;a href=&quot;https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/fouladi&quot;&gt;ExCamera&lt;/a&gt; as two systems that implement specific functions using cloud components (a MapReduce-like framework and video encoding, respectively). In contrast, &lt;em&gt;gg&lt;/em&gt; aims to provide, “common services for dependency management, straggler mitigation, and scheduling.”&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Build tools&lt;/em&gt;: &lt;em&gt;gg&lt;/em&gt; aims to speed up multiple types of applications through parallelization in the cloud. One of those applications, compiling software, is addressed by systems like &lt;a href=&quot;https://bazel.build/&quot;&gt;Bazel&lt;/a&gt;, &lt;a href=&quot;https://www.pantsbuild.org/&quot;&gt;Pants&lt;/a&gt;, and &lt;a href=&quot;https://buck.build&quot;&gt;Buck&lt;/a&gt;. These newer tools are helpful for speeding up builds by parallelizing and incrementalizing operations, but developers will likely not be able to use advanced features of the aforementioned systems unless they rework their existing build.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now that we understand more about the goals of &lt;em&gt;gg&lt;/em&gt;, let’s jump into the system’s design and implementation.&lt;/p&gt;

&lt;h2 id=&quot;design-and-implementation-of-gg&quot;&gt;Design and implementation of gg&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;gg&lt;/em&gt; comprises three main components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;em&gt;gg Intermediate Representation (gg IR)&lt;/em&gt; used to represent the units of computation involved in an application - &lt;em&gt;gg IR&lt;/em&gt; looks like a graph, where dependencies between steps are the edges and the units of computation/data are the nodes.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Frontends&lt;/em&gt;, which take an application and generate the intermediate representation of the program.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Backends&lt;/em&gt;, which execute the &lt;em&gt;gg IR&lt;/em&gt;, store results, and coalesce them when producing output.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/gg/arch.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The &lt;em&gt;gg Intermediate Representation (gg IR)&lt;/em&gt; describes the steps involved in a given execution of an application&lt;label for=&quot;dynamic&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;dynamic&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Notably, this graph is dynamic and lazily evaluated, which is helpful for supporting applications that involve “loops, recursion, or other non-DAG dataflows. &lt;/span&gt;. Each step is described as a &lt;em&gt;thunk&lt;/em&gt;, and includes the command that the step invokes, environment variables, the arguments to that command, and all inputs. Thunks can also be used to represent primitive values that don’t need to be evaluated - for example, binary files like gcc need to be used in the execution of a thunk, but do not need to be executed. A &lt;em&gt;thunk&lt;/em&gt; is identified using a content-addressing scheme&lt;label for=&quot;content&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;content&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The paper describes a content-addressing scheme where, “the name of an object has four components: (1) whether the object is a primitive value (hash starting with V) or represents the result of forcing some other thunk (hash starting with T), (2) a SHA-256 hash, (3) the length in bytes, and (4) an optional tag that names an object or a thunk’s output.” &lt;/span&gt; that allows one &lt;em&gt;thunk&lt;/em&gt; to depend on another (by specifying the objects array as described in the figure below).&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/gg/thunks.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;Frontends&lt;/em&gt; produce the &lt;em&gt;gg IR&lt;/em&gt;, either through a language-specific SDK (where a developer describes an application’s execution in code)&lt;label for=&quot;ray&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;ray&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;This seems like it would have a close connection to &lt;a href=&quot;/2021/06/27/ray-a-distributed-framework-for-emerging-ai-applications.html&quot;&gt;Ray, another previous paper review&lt;/a&gt;. &lt;/span&gt; or with a &lt;em&gt;model substitution primitive&lt;/em&gt;. The model substitution primitive mode uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gg infer&lt;/code&gt; to generate all of the thunks (a.k.a. steps) that would be involved in the execution of the original command. This command executes based on advanced knowledge of how to model specific types of systems - as an example, imagine defining a way to process projects that use &lt;em&gt;make&lt;/em&gt;. In this case, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gg infer&lt;/code&gt; is capable of converting the aforementioned &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;make&lt;/code&gt; command into a set of thunks that will compile independent C++ files in parallel, coalescing the results to produce the intended binary - see the figure below for a visual representation.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/gg/ggir.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;Backends&lt;/em&gt; execute the &lt;em&gt;gg IR&lt;/em&gt; produced by the &lt;em&gt;Frontends&lt;/em&gt; by “forcing” the execution of the thunk that corresponds to the output of the application’s execution. The computation graph is then traced backwards along the edges that lead to the final output. Backends can be implemented on different cloud providers, or even use the developer’s local machine. While the internals of the backends may differ, each backend must have three high-level components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Storage engine&lt;/em&gt;: used to perform CRUD operations for content-addressable outputs (for example, storing the result of a thunk’s execution).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Execution engine&lt;/em&gt;: a function that actually performs the execution of a thunk, abstracting away actual execution. It must support, “a simple abstraction: a function that receives a thunk as the input and returns the hashes of its output objects (which can be either values or thunks)”. Examples of execution engines are “a local multicore machine, a cluster of remote VMs, AWS Lambda, Google Cloud Functions, and IBM Cloud Functions (OpenWhisk)”.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Coordinator&lt;/em&gt;: The coordinator is a process that orchestrates the execution of a &lt;em&gt;gg IR&lt;/em&gt; by communicating with one or more execution engines and the storage engine&lt;label for=&quot;coordinator&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;coordinator&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;It was unclear from the paper whether multiple storage engines can be associated with a single coordinator. &lt;/span&gt;. It provides higher level services like making scheduling decisions, memoizing thunk execution (not rerunning a thunk unnecessarily), rerunning thunks if they fail, and straggler mitigation&lt;label for=&quot;straggler&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;straggler&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Straggler mitigation in this context means ensuring that slow-running &lt;em&gt;thunks&lt;/em&gt; do not impact overall execution time. One strategy to address this issue is uunning multiple copies of a thunk in parallel, then continuing after the first succeds - likely possible because content-addressable nature of thunks means that their execution is idempotent. &lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;applying-and-evaluating-gg&quot;&gt;Applying and evaluating gg&lt;/h2&gt;

&lt;p&gt;The &lt;em&gt;gg&lt;/em&gt; system was applied to, and evaluated against, four&lt;label for=&quot;five&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;five&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The paper also includes an implementation of recursive fibonacci to demonstrate that &lt;em&gt;gg&lt;/em&gt; can handle dynamic execution graphs while also memoizing redundant executions. &lt;/span&gt; use cases: software compilation, unit testing, video encoding, and object recognition.&lt;/p&gt;

&lt;p&gt;For software compilation, FFmpeg, GIMP, Inkscape, and Chromium were compiled either locally, using a distributed build tool (icecc), or with &lt;em&gt;gg&lt;/em&gt;. For medium-to-large programs, (Inkscape and Chromium), &lt;em&gt;gg&lt;/em&gt; performed better than the alternatives with an &lt;em&gt;AWS Lambda&lt;/em&gt; execution engine, likely because it is better able to handle high degrees of parallelism - a &lt;em&gt;gg&lt;/em&gt; based compilation is able to perform all steps remotely, whereas the two other systems perform bottlenecking-steps at the root node. The paper also includes an interesting graphic outlining the behavior of &lt;em&gt;gg&lt;/em&gt; worker’s during compilation, which contains an interesting visual of straggler mitigation (see below).&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/gg/stragglers.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;For unit testing, the LibVPX test suite was built in parallel with &lt;em&gt;gg&lt;/em&gt; on AWS Lambda, and compared with a build box - the time differences between the two strategies was small, but that authors argue that the &lt;em&gt;gg&lt;/em&gt; based solution was able to provide results earlier because of its parallelism.&lt;/p&gt;

&lt;p&gt;For video encoding, &lt;em&gt;gg&lt;/em&gt; performed worse than an optimized implementation (based on ExCamera), although the &lt;em&gt;gg&lt;/em&gt; based system introduces memoization and fault tolerance.&lt;/p&gt;

&lt;p&gt;For object recognition, &lt;em&gt;gg&lt;/em&gt; was compared to &lt;a href=&quot;https://scanner.run&quot;&gt;Scanner&lt;/a&gt;&lt;label for=&quot;scanner&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;scanner&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;“Scanner is a distributed system for building efficient video processing applications that scale.” - it would be interesting to see this implemented in Ray! &lt;/span&gt;, and observed significant speedups&lt;label for=&quot;speedup&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;speedup&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The authors mention that the &lt;em&gt;gg&lt;/em&gt; implementation was specifically tuned to the task. &lt;/span&gt; that the authors attribute to &lt;em&gt;gg&lt;/em&gt;’s scheduling algorithm and removing abstraction in Scanner’s design.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;While &lt;em&gt;gg&lt;/em&gt; seems like an exciting system for scaling command line applications, it may not be the best fit for every project (as indicated by the experimental results) - in particular, &lt;em&gt;gg&lt;/em&gt; seems well positioned to speed up traditional make-based builds without requiring a large-scale migration. The paper authors also note limitations of the system, like &lt;em&gt;gg&lt;/em&gt;’s incompatibility with GPU programs - &lt;a href=&quot;/2021/06/27/ray-a-distributed-framework-for-emerging-ai-applications.html&quot;&gt;my previous paper review on Ray&lt;/a&gt; seems relevant to adapting &lt;em&gt;gg&lt;/em&gt; in the future.&lt;/p&gt;

&lt;p&gt;A quote that I particularly enjoyed from the paper’s conclusion was this:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;As a computing substrate, we suspect cloud functions are in a similar position to Graphics Processing Units in the 2000s. At the time, GPUs were designed solely for 3D graphics, but the community gradually recognized that they had become programmable enough to execute some parallel algorithms unrelated to graphics. Over time, this “general-purpose GPU” (GPGPU) movement created systems-support technologies and became a major use of GPUs, especially for physical simulations and deep neural networks. Cloud functions may tell a similar story. Although intended for asynchronous microservices, we believe that with sufficient effort by this community the same infrastructure is capable of broad and exciting new applications. Just as GPGPU computing did a decade ago, nontraditional “serverless” computing may have far-reaching effects.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Thanks for reading, and feel free to reach out with feedback on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt; - until next time!&lt;/p&gt;</content><author><name>Micah</name></author><summary type="html">This week’s paper review is the second in a series on “The Future of the Shell” (Part 1, a paper about possible ways to innovate in the shell is here). As always, feel free to reach out on Twitter with feedback or suggestions about papers to read! These weekly paper reviews can also be delivered weekly to your inbox.</summary></entry></feed>